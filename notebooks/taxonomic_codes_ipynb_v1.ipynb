{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJD/rCsfYgBqt80E6oYdIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobBurnap/Bioinformatics-MICR4203-MICR5203/blob/main/notebooks/taxonomic_codes_ipynb_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Developing a taxonomic table for taxonomic codes for a diverse set of sequences**"
      ],
      "metadata": {
        "id": "afZn240E8-8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "BIOINFO4/5203 — Colab Exercise Template\n",
        "\n",
        "Use this template for every weekly exercise. It standardizes setup, data paths, and the final summary so grading in Canvas is quick.\n",
        "\n",
        "Workflow\n",
        "\n",
        "    Click the \"Open in Colab\" link in Canvas (points to this notebook in GitHub).\n",
        "    Run Setup cells (installs and mounts Google Drive).\n",
        "    Run the Exercise cells (edit as instructed for each lecture).\n",
        "    Verify the Results Summary prints the values requested by Canvas.\n",
        "    File → Print → Save as PDF and upload .ipynb + PDF to Canvas.\n",
        "\n",
        "    Instructor note (delete in student copy if desired):\n",
        "\n",
        "        Place datasets for this lecture at: Drive → BIOINFO4-5203-F25 → Data → Lxx_topic\n",
        "        Update the constants in Config below: COURSE_DIR, LECTURE_CODE (e.g., L05), and TOPIC.\n",
        "        For heavy jobs (trees, assemblies), provide the PETE output files in the same Data folder so students can analyze them here if the queue is busy.\n",
        "\n"
      ],
      "metadata": {
        "id": "frmCc0lAQcYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto‑setup + course folder (uses your Teaching path)**"
      ],
      "metadata": {
        "id": "bw0BsPOEGVL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A. Mount Google Drive, Import Coding Libraries Necessary for Running Subsequent Code"
      ],
      "metadata": {
        "id": "6JW7liKMC3H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install FIRST, then import\n",
        "%pip install -q biopython       # Install the Biopython package quietly (-q suppresses most output) so we can work with biological sequence files\n",
        "\n",
        "from google.colab import drive  # Import the module that lets Colab interact with Google Drive\n",
        "drive.mount('/content/drive')   # Mount your Google Drive so it appears in Colab's file system under /content/drive\n",
        "\n",
        "import os, pandas as pd          # Import 'os' for file/directory operations, and pandas for working with data tables\n",
        "from Bio import SeqIO            # Import SeqIO from Biopython for reading/writing biological sequence files (FASTA, GenBank, etc.)\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib's plotting library to create figures and graphs\n",
        "\n",
        "print(\"✅ Dependencies installed & Drive mounted.\")\n"
      ],
      "metadata": {
        "id": "N1NrV2YaGfZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08480d0-c264-4034-aecb-a3994ce6775d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m2.5/3.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "✅ Dependencies installed & Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## B. Course folders: Define the course folders for places to load data to be processed and output to be saved\n",
        "\n",
        "Edit only `LECTURE_CODE` and `TOPIC` if needed. All inputs will live in `Data/LECTURE_TOPIC` and outputs in `Outputs/LECTURE_TOPIC`."
      ],
      "metadata": {
        "id": "j7uJVzrKC-8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Define file sturcture"
      ],
      "metadata": {
        "id": "onPHWY4dGku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Course folder config (customize LECTURE_CODE/TOPIC only) ---\n",
        "COURSE_DIR   = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "LECTURE_CODE = \"Taxonomy\"            # change per week (e.g., L02, L03, ...)\n",
        "TOPIC        = \"Template\"    # short slug for the exercise\n",
        "\n",
        "# Derived paths (do not change)\n",
        "DATA_DIR   = f\"{COURSE_DIR}/Data/{LECTURE_CODE}_{TOPIC}\"\n",
        "OUTPUT_DIR = f\"{COURSE_DIR}/Outputs/{LECTURE_CODE}_{TOPIC}\"\n",
        "\n",
        "# Create folder structure if missing\n",
        "for p in [f\"{COURSE_DIR}/Data\", f\"{COURSE_DIR}/Outputs\", f\"{COURSE_DIR}/Notebooks\", DATA_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"📁 COURSE_DIR :\", COURSE_DIR)\n",
        "print(\"📁 DATA_DIR   :\", DATA_DIR)\n",
        "print(\"📁 OUTPUT_DIR :\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1wyBalSGox_",
        "outputId": "bdfac62d-05a6-460f-e9e0-0b4d39414e24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 COURSE_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\n",
            "📁 DATA_DIR   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/Taxonomy_Template\n",
            "📁 OUTPUT_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Take file name \"taxonomic_cmmon_names\" and find taxonomic ids"
      ],
      "metadata": {
        "id": "H1Qe2LFAFz5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Parse FASTA → CSV, then plot a quick figure → PNG in Outputs/**"
      ],
      "metadata": {
        "id": "ccijET3tGsBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Read 'taxonomic_common_names' from DATA_DIR, resolve to TaxIDs (incl. common names), merge & write outputs ---\n",
        "from Bio import Entrez\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import csv, io, re, time, sys, os\n",
        "\n",
        "# REQUIRED\n",
        "Entrez.email = \"you@university.edu\"   # <-- set your email here\n",
        "\n",
        "# Expect DATA_DIR and OUTPUT_DIR from your course scaffold\n",
        "assert 'DATA_DIR' in globals() and 'OUTPUT_DIR' in globals(), \"Run your course folder setup cell first.\"\n",
        "DATA_DIR = Path(DATA_DIR)\n",
        "OUT      = Path(OUTPUT_DIR)\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Locate the names file (exact name requested) ----\n",
        "# Accept either no extension, .txt, .csv, or .tsv\n",
        "CANDIDATES = [\n",
        "    DATA_DIR / \"taxonomic_common_names\",\n",
        "    DATA_DIR / \"taxonomic_common_names.txt\",\n",
        "    DATA_DIR / \"taxonomic_common_names.csv\",\n",
        "    DATA_DIR / \"taxonomic_c0mmon_names.tsv\",\n",
        "]\n",
        "NAMES_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
        "if not NAMES_PATH:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find 'taxonomic_cmmon_names' in {DATA_DIR}. \"\n",
        "        \"Create it as a plain text/CSV/TSV file (one name per line; first column if table).\"\n",
        "    )\n",
        "print(f\"📄 Names file: {NAMES_PATH}\")\n",
        "\n",
        "# ---- Load names (plain text OR first column of CSV/TSV). Lines starting with # are ignored. ----\n",
        "def load_names(path: Path):\n",
        "    text = path.read_text(errors=\"ignore\")\n",
        "    # Heuristic: treat as table if we see a comma or tab in first few lines\n",
        "    head = \"\\n\".join(text.splitlines()[:5])\n",
        "    is_table = (\",\" in head) or (\"\\t\" in head)\n",
        "    names = []\n",
        "    if is_table:\n",
        "        # Try TSV first if tabs present, else CSV\n",
        "        sep = \"\\t\" if \"\\t\" in head else \",\"\n",
        "        df = pd.read_csv(io.StringIO(text), sep=sep, comment=\"#\", header=None)\n",
        "        # take first column\n",
        "        col = df.columns[0]\n",
        "        names = [str(x).strip() for x in df[col].tolist() if str(x).strip()]\n",
        "    else:\n",
        "        for line in text.splitlines():\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            names.append(line)\n",
        "    return names\n",
        "\n",
        "raw_names = load_names(NAMES_PATH)\n",
        "if not raw_names:\n",
        "    raise ValueError(f\"No names found in {NAMES_PATH} (file empty or only comments).\")\n",
        "print(f\"📝 Loaded {len(raw_names)} name(s)\")\n",
        "\n",
        "# ---- Resolve name -> TaxID (try scientific first, then common name, then loose) ----\n",
        "def tax_lookup(name: str):\n",
        "    \"\"\"\n",
        "    Resolve 'name' (scientific or common) -> (taxid, canonical, rank, match_type, status)\n",
        "    match_type: SCIN / COMMON / LOOSE\n",
        "    status: 'ok' or reason\n",
        "    \"\"\"\n",
        "    name = name.strip()\n",
        "    # If user accidentally included a TaxID, accept it directly\n",
        "    if re.fullmatch(r\"\\d+\", name):\n",
        "        try:\n",
        "            tid = int(name)\n",
        "            h2 = Entrez.efetch(db=\"taxonomy\", id=str(tid), retmode=\"xml\"); rec = Entrez.read(h2); h2.close()\n",
        "            node = rec[0]\n",
        "            return tid, node.get(\"ScientificName\",\"\"), node.get(\"Rank\",\"\"), \"TAXID\", \"ok\"\n",
        "        except Exception as e:\n",
        "            return \"\", \"\", \"\", \"TAXID\", f\"efetch_failed:{e}\"\n",
        "\n",
        "    queries = [\n",
        "        (f\"\\\"{name}\\\"[SCIN]\", \"SCIN\"),               # strict scientific name\n",
        "        (f\"\\\"{name}\\\"[Common Name]\", \"COMMON\"),     # strict common name\n",
        "        (name, \"LOOSE\"),                            # loose text search (all-name fields)\n",
        "    ]\n",
        "    tried = set()\n",
        "    for q, tag in queries:\n",
        "        if q in tried:\n",
        "            continue\n",
        "        tried.add(q)\n",
        "        try:\n",
        "            h = Entrez.esearch(db=\"taxonomy\", term=q, retmode=\"xml\"); r = Entrez.read(h); h.close()\n",
        "            if int(r[\"Count\"]) == 0:\n",
        "                continue\n",
        "            tid = r[\"IdList\"][0]\n",
        "            h2 = Entrez.efetch(db=\"taxonomy\", id=tid, retmode=\"xml\"); rec = Entrez.read(h2); h2.close()\n",
        "            node = rec[0]\n",
        "            canon = node.get(\"ScientificName\",\"\")\n",
        "            rank  = node.get(\"Rank\",\"\")\n",
        "            return int(tid), canon, rank, tag, \"ok\"\n",
        "        except Exception as e:\n",
        "            sys.stderr.write(f\"[warn] lookup '{name}' via {tag}: {e}\\n\")\n",
        "            continue\n",
        "    return \"\", \"\", \"\", \"NA\", \"no_match\"\n",
        "\n",
        "rows = []\n",
        "seen = set()\n",
        "for nm in raw_names:\n",
        "    nm = nm.strip()\n",
        "    if not nm or nm.lower() in seen:\n",
        "        continue\n",
        "    seen.add(nm.lower())\n",
        "    tid, canon, rank, tag, status = tax_lookup(nm)\n",
        "    rows.append({\n",
        "        \"input_name\": nm,\n",
        "        \"taxid\": tid,\n",
        "        \"canonical_name\": canon,\n",
        "        \"rank\": rank,\n",
        "        \"match_type\": tag,\n",
        "        \"status\": status\n",
        "    })\n",
        "    time.sleep(0.25)  # be polite to NCBI\n",
        "\n",
        "df_new = pd.DataFrame(rows).sort_values([\"status\",\"canonical_name\",\"input_name\"], na_position=\"last\")\n",
        "map_path = OUT / \"augmented_taxon_map.csv\"\n",
        "df_new.to_csv(map_path, index=False)\n",
        "print(f\"🗺  Wrote map: {map_path}\")\n",
        "\n",
        "# ---- Merge with existing TaxID lists if present ----\n",
        "existing_taxids = set()\n",
        "for p in [OUT/\"taxids.txt\", OUT/\"taxids-augmented.txt\", DATA_DIR/\"taxids.txt\"]:\n",
        "    if p.exists():\n",
        "        existing_taxids |= {t.strip() for t in p.read_text().splitlines() if t.strip().isdigit()}\n",
        "\n",
        "new_taxids = {str(t) for t in df_new[\"taxid\"].tolist() if str(t).isdigit()}\n",
        "all_taxids = sorted(existing_taxids | new_taxids, key=lambda x: int(x))\n",
        "\n",
        "# ---- Write merged outputs ----\n",
        "(OUT / \"taxids-augmented.txt\").write_text(\"\\n\".join(all_taxids))\n",
        "eq_exp = \" OR \".join([f\"txid{t}[ORGN]\" for t in all_taxids])  # [ORGN] is robust in BLAST\n",
        "(OUT / \"entrez_query_exp_aug.txt\").write_text(eq_exp)\n",
        "\n",
        "# ---- Report unresolved names (if any) ----\n",
        "unresolved = df_new[df_new[\"status\"] != \"ok\"]\n",
        "if not unresolved.empty:\n",
        "    bad_list = \", \".join(unresolved[\"input_name\"].tolist()[:12])\n",
        "    print(f\"⚠️ Unresolved names: {len(unresolved)} (showing a few): {bad_list}\")\n",
        "\n",
        "print(f\"✅ Resolved {len(new_taxids)} new TaxIDs. Total in merged list: {len(all_taxids)}\")\n",
        "print(\"🧬 TaxIDs :\", OUT / 'taxids-augmented.txt')\n",
        "print(\"🔎 ENTREQ :\", OUT / 'entrez_query_exp_aug.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y14FsfkKF4f5",
        "outputId": "e0e18159-fd8c-4e50-fdc7-e0fcdd8496e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Names file: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/Taxonomy_Template/taxonomic_common_names.txt\n",
            "📝 Loaded 127 name(s)\n",
            "🗺  Wrote map: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/augmented_taxon_map.csv\n",
            "⚠️ Unresolved names: 1 (showing a few): Candidatus Syntrophoarchaeum butanivorans\n",
            "✅ Resolved 123 new TaxIDs. Total in merged list: 123\n",
            "🧬 TaxIDs : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/taxids-augmented.txt\n",
            "🔎 ENTREQ : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/entrez_query_exp_aug.txt\n"
          ]
        }
      ]
    }
  ]
}