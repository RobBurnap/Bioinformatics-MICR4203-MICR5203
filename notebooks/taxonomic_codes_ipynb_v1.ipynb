{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlhEJ/RlUBE4Sn7UH34Udm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobBurnap/Bioinformatics-MICR4203-MICR5203/blob/main/notebooks/taxonomic_codes_ipynb_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Developing a taxonomic table for taxonomic codes for a diverse set of sequences**"
      ],
      "metadata": {
        "id": "afZn240E8-8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "BIOINFO4/5203 â€” Colab Exercise Template\n",
        "\n",
        "Use this template for every weekly exercise. It standardizes setup, data paths, and the final summary so grading in Canvas is quick.\n",
        "\n",
        "Workflow\n",
        "\n",
        "    Click the \"Open in Colab\" link in Canvas (points to this notebook in GitHub).\n",
        "    Run Setup cells (installs and mounts Google Drive).\n",
        "    Run the Exercise cells (edit as instructed for each lecture).\n",
        "    Verify the Results Summary prints the values requested by Canvas.\n",
        "    File â†’ Print â†’ Save as PDF and upload .ipynb + PDF to Canvas.\n",
        "\n",
        "    Instructor note (delete in student copy if desired):\n",
        "\n",
        "        Place datasets for this lecture at: Drive â†’ BIOINFO4-5203-F25 â†’ Data â†’ Lxx_topic\n",
        "        Update the constants in Config below: COURSE_DIR, LECTURE_CODE (e.g., L05), and TOPIC.\n",
        "        For heavy jobs (trees, assemblies), provide the PETE output files in the same Data folder so students can analyze them here if the queue is busy.\n",
        "\n"
      ],
      "metadata": {
        "id": "frmCc0lAQcYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autoâ€‘setup + course folder (uses your Teaching path)**"
      ],
      "metadata": {
        "id": "bw0BsPOEGVL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A. Mount Google Drive, Import Coding Libraries Necessary for Running Subsequent Code"
      ],
      "metadata": {
        "id": "6JW7liKMC3H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install FIRST, then import\n",
        "%pip install -q biopython       # Install the Biopython package quietly (-q suppresses most output) so we can work with biological sequence files\n",
        "\n",
        "from google.colab import drive  # Import the module that lets Colab interact with Google Drive\n",
        "drive.mount('/content/drive')   # Mount your Google Drive so it appears in Colab's file system under /content/drive\n",
        "\n",
        "import os, pandas as pd          # Import 'os' for file/directory operations, and pandas for working with data tables\n",
        "from Bio import SeqIO            # Import SeqIO from Biopython for reading/writing biological sequence files (FASTA, GenBank, etc.)\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib's plotting library to create figures and graphs\n",
        "\n",
        "print(\"âœ… Dependencies installed & Drive mounted.\")\n"
      ],
      "metadata": {
        "id": "N1NrV2YaGfZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf17de1-719c-4470-85e3-f5df060566b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/3.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "âœ… Dependencies installed & Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## B. Course folders: Define the course folders for places to load data to be processed and output to be saved\n",
        "\n",
        "Edit only `LECTURE_CODE` and `TOPIC` if needed. All inputs will live in `Data/LECTURE_TOPIC` and outputs in `Outputs/LECTURE_TOPIC`."
      ],
      "metadata": {
        "id": "j7uJVzrKC-8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Define file sturcture"
      ],
      "metadata": {
        "id": "onPHWY4dGku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Course folder config (customize LECTURE_CODE/TOPIC only) ---\n",
        "COURSE_DIR   = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "LECTURE_CODE = \"Taxonomy\"            # change per week (e.g., L02, L03, ...)\n",
        "TOPIC        = \"Template\"    # short slug for the exercise\n",
        "\n",
        "# Derived paths (do not change)\n",
        "DATA_DIR   = f\"{COURSE_DIR}/Data/{LECTURE_CODE}_{TOPIC}\"\n",
        "OUTPUT_DIR = f\"{COURSE_DIR}/Outputs/{LECTURE_CODE}_{TOPIC}\"\n",
        "\n",
        "# Create folder structure if missing\n",
        "for p in [f\"{COURSE_DIR}/Data\", f\"{COURSE_DIR}/Outputs\", f\"{COURSE_DIR}/Notebooks\", DATA_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"ðŸ“ COURSE_DIR :\", COURSE_DIR)\n",
        "print(\"ðŸ“ DATA_DIR   :\", DATA_DIR)\n",
        "print(\"ðŸ“ OUTPUT_DIR :\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1wyBalSGox_",
        "outputId": "c02b07c9-e151-4af6-9fee-74daaaabc1de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ COURSE_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\n",
            "ðŸ“ DATA_DIR   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/Taxonomy_Template\n",
            "ðŸ“ OUTPUT_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Take file name \"taxonomic_cmmon_names\" and find taxonomic ids"
      ],
      "metadata": {
        "id": "H1Qe2LFAFz5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Version 3 Aug 25"
      ],
      "metadata": {
        "id": "ccijET3tGsBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Resolve names -> NCBI TaxIDs, add Domain + Phylum, and write mapping + helper files ---\n",
        "from Bio import Entrez\n",
        "from pathlib import Path\n",
        "import pandas as pd, io, re, os, sys, time\n",
        "\n",
        "# REQUIRED: set your email; an API key is optional but helpful for rate limits\n",
        "Entrez.email = \"you@university.edu\"\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"   # <- optional\n",
        "\n",
        "# Expect these from your course scaffold cell\n",
        "assert 'DATA_DIR' in globals() and 'OUTPUT_DIR' in globals(), \"Run your course folder setup cell first.\"\n",
        "DATA_DIR = Path(DATA_DIR); OUT = Path(OUTPUT_DIR)\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Find your names file (one per line; scientific or common names are fine) ----\n",
        "CANDIDATES = [\n",
        "    DATA_DIR / \"taxonomic_common_names\",\n",
        "    DATA_DIR / \"taxonomic_common_names.txt\",\n",
        "    DATA_DIR / \"taxonomic_common_names.csv\",\n",
        "    DATA_DIR / \"taxonomic_common_names.tsv\",\n",
        "]\n",
        "NAMES_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
        "if not NAMES_PATH:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find 'taxonomic_common_names' (txt/csv/tsv) in {DATA_DIR}.\\n\"\n",
        "        \"Create it with one name per line (scientific or common).\"\n",
        "    )\n",
        "print(\"ðŸ“„ Names file:\", NAMES_PATH)\n",
        "\n",
        "# ---- Load names (plain text or first column of CSV/TSV). Lines starting with # are ignored. ----\n",
        "def load_names(path: Path):\n",
        "    text = path.read_text(errors=\"ignore\")\n",
        "    head = \"\\n\".join(text.splitlines()[:5])\n",
        "    is_table = (\",\" in head) or (\"\\t\" in head)\n",
        "    names = []\n",
        "    if is_table:\n",
        "        sep = \"\\t\" if \"\\t\" in head else \",\"\n",
        "        df = pd.read_csv(io.StringIO(text), sep=sep, comment=\"#\", header=None)\n",
        "        col0 = df.columns[0]\n",
        "        names = [str(x).strip() for x in df[col0].tolist() if str(x).strip()]\n",
        "    else:\n",
        "        for line in text.splitlines():\n",
        "            line = line.strip()\n",
        "            if line and not line.startswith(\"#\"):\n",
        "                names.append(line)\n",
        "    return names\n",
        "\n",
        "raw_names = load_names(NAMES_PATH)\n",
        "print(f\"ðŸ“ Loaded {len(raw_names)} name(s)\")\n",
        "\n",
        "# ---- Helpers to fetch taxonomy and extract lineage ranks ----\n",
        "def fetch_taxnode_by_id(tid: str):\n",
        "    h2 = Entrez.efetch(db=\"taxonomy\", id=tid, retmode=\"xml\")\n",
        "    rec = Entrez.read(h2); h2.close()\n",
        "    return rec[0] if rec else {}\n",
        "\n",
        "def lineage_map(node):\n",
        "    \"\"\"Return dict rank->scientificName from LineageEx (e.g., {'superkingdom':'Bacteria','phylum':'Proteobacteria',...}).\"\"\"\n",
        "    lm = {}\n",
        "    for x in node.get(\"LineageEx\", []):\n",
        "        rk = x.get(\"Rank\"); nm = x.get(\"ScientificName\")\n",
        "        if rk and nm:\n",
        "            lm[rk] = nm\n",
        "    return lm\n",
        "\n",
        "def tax_lookup(name: str):\n",
        "    \"\"\"\n",
        "    Resolve 'name' (scientific, common, or numeric TaxID) â†’\n",
        "      (taxid, canonical_name, rank, domain, phylum, match_type, status)\n",
        "    match_type: SCIN | COMMON | LOOSE | TAXID\n",
        "    status: 'ok' or reason\n",
        "    \"\"\"\n",
        "    name = name.strip()\n",
        "\n",
        "    # direct TaxID?\n",
        "    if re.fullmatch(r\"\\d+\", name):\n",
        "        try:\n",
        "            node = fetch_taxnode_by_id(name)\n",
        "            lm = lineage_map(node)\n",
        "            dom = lm.get(\"superkingdom\", \"NA\")\n",
        "            phylum = lm.get(\"phylum\", \"NA\")\n",
        "            return int(name), node.get(\"ScientificName\",\"\"), node.get(\"Rank\",\"\"), dom, phylum, \"TAXID\", \"ok\"\n",
        "        except Exception as e:\n",
        "            return \"\", \"\", \"\", \"NA\", \"NA\", \"TAXID\", f\"efetch_failed:{e}\"\n",
        "\n",
        "    queries = [\n",
        "        (f\"\\\"{name}\\\"[SCIN]\",        \"SCIN\"),     # exact scientific name\n",
        "        (f\"\\\"{name}\\\"[Common Name]\", \"COMMON\"),   # exact common name\n",
        "        (name,                       \"LOOSE\"),    # loose text search\n",
        "    ]\n",
        "    for q, tag in queries:\n",
        "        try:\n",
        "            h = Entrez.esearch(db=\"taxonomy\", term=q, retmode=\"xml\"); r = Entrez.read(h); h.close()\n",
        "            if int(r[\"Count\"]) == 0:\n",
        "                continue\n",
        "            tid = r[\"IdList\"][0]\n",
        "            node = fetch_taxnode_by_id(tid)\n",
        "            lm = lineage_map(node)\n",
        "            dom = lm.get(\"superkingdom\", \"NA\")\n",
        "            phylum = lm.get(\"phylum\", \"NA\")\n",
        "            return int(tid), node.get(\"ScientificName\",\"\"), node.get(\"Rank\",\"\"), dom, phylum, tag, \"ok\"\n",
        "        except Exception as e:\n",
        "            sys.stderr.write(f\"[warn] lookup '{name}' via {tag}: {e}\\n\")\n",
        "            time.sleep(0.15)\n",
        "            continue\n",
        "    return \"\", \"\", \"\", \"NA\", \"NA\", \"NA\", \"no_match\"\n",
        "\n",
        "# ---- Resolve all entries ----\n",
        "rows = []\n",
        "seen = set()\n",
        "for nm in raw_names:\n",
        "    key = nm.strip().lower()\n",
        "    if not key or key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    tid, canon, rank, dom, phylum, tag, status = tax_lookup(nm)\n",
        "    rows.append({\n",
        "        \"input_name\": nm,\n",
        "        \"taxid\": tid,\n",
        "        \"organism_name\": canon,      # explicit column as requested (canonical scientific name)\n",
        "        \"canonical_name\": canon,     # keep for continuity with earlier files\n",
        "        \"rank\": rank,\n",
        "        \"domain\": dom,\n",
        "        \"phylum\": phylum,\n",
        "        \"match_type\": tag,\n",
        "        \"status\": status\n",
        "    })\n",
        "    time.sleep(0.25)  # be polite to NCBI; increase if you hit rate limits\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.sort_values(\n",
        "    [\"status\",\"domain\",\"phylum\",\"organism_name\",\"input_name\"],\n",
        "    na_position=\"last\"\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# ---- Write outputs ----\n",
        "map_csv       = OUT / \"taxon_map.csv\"                # full mapping table (with organism_name + phylum)\n",
        "ids_txt       = OUT / \"taxids_list.txt\"              # one TaxID per line\n",
        "entrez_txt    = OUT / \"entrez_query.txt\"             # txidXXX[ORGN] OR ...\n",
        "gui_taxids    = OUT / \"blast_gui_taxids.csv\"         # comma-separated numeric IDs for GUI (enter one-by-one via \"Add organism\")\n",
        "gui_names_txt = OUT / \"blast_gui_names.txt\"          # comma-separated canonical names (GUI-friendly labels)\n",
        "\n",
        "df.to_csv(map_csv, index=False)\n",
        "\n",
        "valid_taxids = [str(t) for t in df[\"taxid\"].tolist() if str(t).isdigit()]\n",
        "ids_txt.write_text(\"\\n\".join(valid_taxids))\n",
        "entrez_txt.write_text(\" OR \".join([f\"txid{t}[ORGN]\" for t in valid_taxids]))\n",
        "\n",
        "# For the web GUI: names and IDs as comma-separated lines (note: GUI prefers adding names one at a time)\n",
        "canon_names = [n for n in df[\"organism_name\"].tolist() if n]\n",
        "gui_taxids.write_text(\", \".join(valid_taxids))\n",
        "gui_names_txt.write_text(\", \".join(canon_names))\n",
        "\n",
        "print(\"ðŸ—º  Map CSV :\", map_csv)\n",
        "print(\"ðŸ§¬ TaxIDs  :\", ids_txt)\n",
        "print(\"ðŸ”Ž ENTREQ  :\", entrez_txt)\n",
        "print(\"ðŸ§© GUI IDs :\", gui_taxids)\n",
        "print(\"ðŸ§© GUI names:\", gui_names_txt)\n",
        "\n",
        "# ---- Summary ----\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(\"Total names read :\", len(raw_names))\n",
        "print(\"Resolved (ok)    :\", int((df[\"status\"]==\"ok\").sum()))\n",
        "print(\"Unresolved       :\", int((df[\"status\"]!=\"ok\").sum()))\n",
        "if (df[\"status\"]!=\"ok\").any():\n",
        "    print(df[df[\"status\"]!=\"ok\"][[\"input_name\",\"status\"]].head(12))"
      ],
      "metadata": {
        "id": "H2dejbPgSSSg",
        "outputId": "75065ea4-6c05-4098-950b-fd1a4e915389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“„ Names file: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/Taxonomy_Template/taxonomic_common_names.txt\n",
            "ðŸ“ Loaded 127 name(s)\n",
            "ðŸ—º  Map CSV : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/taxon_map.csv\n",
            "ðŸ§¬ TaxIDs  : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/taxids_list.txt\n",
            "ðŸ”Ž ENTREQ  : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/entrez_query.txt\n",
            "ðŸ§© GUI IDs : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/blast_gui_taxids.csv\n",
            "ðŸ§© GUI names: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/blast_gui_names.txt\n",
            "\n",
            "=== Summary ===\n",
            "Total names read : 127\n",
            "Resolved (ok)    : 123\n",
            "Unresolved       : 1\n",
            "                                  input_name    status\n",
            "0  Candidatus Syntrophoarchaeum butanivorans  no_match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fasta Header Manipulation**"
      ],
      "metadata": {
        "id": "GAyod7MQ77Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Rewrite FASTA headers to \">Phylum Scientific_name\" using TaxID lookups ---\n",
        "from Bio import Entrez, SeqIO\n",
        "from pathlib import Path\n",
        "import re, sys, time, csv\n",
        "\n",
        "# ====== REQUIRED: set your email (and optionally API key) ======\n",
        "Entrez.email = \"you@university.edu\"     # <-- put a real email here\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"  # optional\n",
        "\n",
        "# ====== I/O paths ======\n",
        "# If your course scaffold defined these, weâ€™ll use them; else fall back to local files.\n",
        "IN_FASTA  = None\n",
        "OUT_FASTA = None\n",
        "if 'DATA_DIR' in globals() and 'OUTPUT_DIR' in globals():\n",
        "    DATA_DIR  = Path(DATA_DIR)\n",
        "    OUTPUT_DIR = Path(OUTPUT_DIR); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    IN_FASTA  = IN_FASTA  or (OUTPUT_DIR / \"per_taxid_top_hits.fasta\")\n",
        "    OUT_FASTA = OUT_FASTA or (OUTPUT_DIR / \"per_taxid_top_hits.phylum_name.fasta\")\n",
        "else:\n",
        "    # Fallback: edit these two lines if not running in the course scaffold\n",
        "    IN_FASTA  = Path(\"per_taxid_top_hits.fasta\")\n",
        "    OUT_FASTA = Path(\"per_taxid_top_hits.phylum_name.fasta\")\n",
        "\n",
        "CACHE_CSV = OUT_FASTA.with_suffix(\".taxid_map.csv\")   # saves a small map for reference\n",
        "\n",
        "# ====== utils ======\n",
        "_tax_cache = {}  # tid -> (phylum, scientific_name)\n",
        "\n",
        "def fetch_taxnode_by_id(tid: str):\n",
        "    \"\"\"Return taxonomy record (dict) for a TaxID.\"\"\"\n",
        "    h = Entrez.efetch(db=\"taxonomy\", id=str(tid), retmode=\"xml\")\n",
        "    rec = Entrez.read(h); h.close()\n",
        "    return rec[0] if rec else {}\n",
        "\n",
        "def lineage_rank(node, rank_name):\n",
        "    \"\"\"Get lineage scientific name at given rank (e.g., 'phylum', 'superkingdom').\"\"\"\n",
        "    for x in node.get(\"LineageEx\", []):\n",
        "        if x.get(\"Rank\") == rank_name:\n",
        "            return x.get(\"ScientificName\")\n",
        "    return None\n",
        "\n",
        "def phylum_and_name_for_taxid(tid: str):\n",
        "    \"\"\"Resolve TaxID -> (phylum, canonical scientific name). Cached.\"\"\"\n",
        "    if tid in _tax_cache:\n",
        "        return _tax_cache[tid]\n",
        "    try:\n",
        "        node = fetch_taxnode_by_id(tid)\n",
        "        phylum = lineage_rank(node, \"phylum\") or \"NA\"\n",
        "        sci    = node.get(\"ScientificName\", \"Unknown\")\n",
        "        _tax_cache[tid] = (phylum, sci)\n",
        "        return phylum, sci\n",
        "    except Exception as e:\n",
        "        sys.stderr.write(f\"[warn] taxid {tid} lookup failed: {e}\\n\")\n",
        "        _tax_cache[tid] = (\"NA\", \"Unknown\")\n",
        "        return \"NA\", \"Unknown\"\n",
        "\n",
        "def extract_taxid_from_header(header: str):\n",
        "    \"\"\"Find taxid:#### in a FASTA header; return digits or None.\"\"\"\n",
        "    m = re.search(r\"taxid:(\\d+)\", header)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# ====== main ======\n",
        "if not IN_FASTA.exists():\n",
        "    print(f\"âŒ Error: Input FASTA not found at {IN_FASTA}\")\n",
        "    print(\"Please ensure the file exists or is generated by a previous step.\")\n",
        "    raise FileNotFoundError(f\"Input FASTA not found: {IN_FASTA}\")\n",
        "\n",
        "records_in  = list(SeqIO.parse(str(IN_FASTA), \"fasta\"))\n",
        "if not records_in:\n",
        "    raise ValueError(f\"No records found in {IN_FASTA}\")\n",
        "\n",
        "# Resolve all unique TaxIDs first (polite rate limiting)\n",
        "taxids = []\n",
        "for r in records_in:\n",
        "    tid = extract_taxid_from_header(r.description)\n",
        "    if tid:\n",
        "        taxids.append(tid)\n",
        "uniq_taxids = sorted(set(taxids), key=int)\n",
        "\n",
        "print(f\"ðŸ”Ž Found {len(uniq_taxids)} unique TaxIDs in {IN_FASTA.name}\")\n",
        "for i, tid in enumerate(uniq_taxids, 1):\n",
        "    ph, nm = phylum_and_name_for_taxid(tid)\n",
        "    if i % 5 == 0:    # gentle pacing\n",
        "        time.sleep(0.2)\n",
        "\n",
        "# Rewrite headers and write output FASTA\n",
        "written = 0\n",
        "with open(OUT_FASTA, \"w\") as out_fa:\n",
        "    for r in records_in:\n",
        "        tid = extract_taxid_from_header(r.description)\n",
        "        if not tid:\n",
        "            ph, nm = \"NA\", \"Unknown\"\n",
        "        else:\n",
        "            ph, nm = phylum_and_name_for_taxid(tid)\n",
        "        # EXACT requested format:\n",
        "        r.id = f\"{ph} {nm}\"\n",
        "        r.name = \"\"\n",
        "        r.description = \"\"   # keep header clean as just \">Phylum Scientific_name\"\n",
        "        SeqIO.write(r, out_fa, \"fasta\")\n",
        "        written += 1\n",
        "\n",
        "# Save a small mapping file for audit/reference\n",
        "with open(CACHE_CSV, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"taxid\",\"phylum\",\"scientific_name\"])\n",
        "    for tid in uniq_taxids:\n",
        "        ph, nm = _tax_cache.get(tid, (\"NA\",\"Unknown\"))\n",
        "        w.writerow([tid, ph, nm])\n",
        "\n",
        "print(f\"âœ… Wrote {written} sequences to: {OUT_FASTA}\")\n",
        "print(f\"ðŸ—º  Saved TaxIDâ†’(phylum,name) map: {CACHE_CSV}\")\n",
        "\n",
        "# ---- Optional note on duplicate headers ----\n",
        "if len(uniq_taxids) < len(records_in):\n",
        "    # Many seqs may collapse to identical headers if same species; thatâ€™s okay for MSA tools that only need sequences.\n",
        "    # If you need unique headers, uncomment the two lines below to append an incrementing number.\n",
        "    # for k, rec in enumerate(SeqIO.parse(str(OUT_FASTA), \"fasta\"), 1): ...\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPS0sRiQ8CHZ",
        "outputId": "865354b9-d08a-4199-bb55-d4ed4de6aac1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”Ž Found 47 unique TaxIDs in per_taxid_top_hits.fasta\n",
            "âœ… Wrote 77 sequences to: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/per_taxid_top_hits.phylum_name.fasta\n",
            "ðŸ—º  Saved TaxIDâ†’(phylum,name) map: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/per_taxid_top_hits.phylum_name.taxid_map.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HTML** **generator**"
      ],
      "metadata": {
        "id": "9xSEj2JLTlq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Merge 1+ FASTAs, resolve TaxID for each seq (from header or by accession),\n",
        "# --- then rewrite headers to \">Phylum Scientific_name\" and write *all* records.\n",
        "from Bio import Entrez, SeqIO\n",
        "from pathlib import Path\n",
        "import re, sys, time, csv, hashlib\n",
        "\n",
        "# ====== REQUIRED ======\n",
        "Entrez.email = \"you@university.edu\"      # put a real email here\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"   # optional but recommended\n",
        "\n",
        "# ====== Inputs ======\n",
        "# If your course scaffold exists, we auto-pick common files. You can add more.\n",
        "IN_FASTAS = []\n",
        "if 'DATA_DIR' in globals() and 'OUTPUT_DIR' in globals():\n",
        "    DATA_DIR   = Path(DATA_DIR)\n",
        "    OUTPUT_DIR = Path(OUTPUT_DIR); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    # Add any that exist:\n",
        "    for cand in [\n",
        "        OUTPUT_DIR / \"per_taxid_top_hits.fasta\",\n",
        "        DATA_DIR   / \"blast_hits.fasta\",          # from earlier XMLâ†’FASTA step\n",
        "        DATA_DIR   / \"query_proteins.fasta\",      # in case you want to include the query too\n",
        "    ]:\n",
        "        if cand.exists(): IN_FASTAS.append(cand)\n",
        "    OUT_FASTA = OUTPUT_DIR / \"all_hits.phylum_name.fasta\"\n",
        "else:\n",
        "    # Fallback: edit these paths as needed\n",
        "    IN_FASTAS = [Path(\"per_taxid_top_hits.fasta\"), Path(\"blast_hits.fasta\")]\n",
        "    OUT_FASTA = Path(\"all_hits.phylum_name.fasta\")\n",
        "\n",
        "if not IN_FASTAS:\n",
        "    raise FileNotFoundError(\"No input FASTA files found. Add paths to IN_FASTAS.\")\n",
        "\n",
        "MAP_CSV = OUT_FASTA.with_suffix(\".taxid_map.csv\")\n",
        "\n",
        "# ====== helpers ======\n",
        "_tax_cache = {}   # tid -> (phylum, sci)\n",
        "_acc2tid   = {}   # accession -> taxid\n",
        "\n",
        "DNA = set(\"ACGTNUWSMKRYBDHV-\")\n",
        "\n",
        "def extract_taxid_from_header(header: str):\n",
        "    m = re.search(r\"taxid:(\\d+)\", header)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def first_accession_token(header: str):\n",
        "    \"\"\"\n",
        "    Try to pull an accession token from the header (e.g., 'WP_012345678.1', 'XOB97566.1').\n",
        "    We use first whitespace-delimited token that looks like an accession.\n",
        "    \"\"\"\n",
        "    tok = header.split()[0]\n",
        "    # (very loose) accession shape: letters/underscore + digits + optional .version\n",
        "    if re.match(r\"^[A-Za-z]{1,6}[_]?\\d+(?:\\.\\d+)?$\", tok):\n",
        "        return tok\n",
        "    return None\n",
        "\n",
        "def fetch_taxnode_by_id(tid: str):\n",
        "    h = Entrez.efetch(db=\"taxonomy\", id=str(tid), retmode=\"xml\")\n",
        "    rec = Entrez.read(h); h.close()\n",
        "    return rec[0] if rec else {}\n",
        "\n",
        "def lineage_rank(node, rank_name):\n",
        "    for x in node.get(\"LineageEx\", []):\n",
        "        if x.get(\"Rank\") == rank_name:\n",
        "            return x.get(\"ScientificName\")\n",
        "    return None\n",
        "\n",
        "def phylum_and_name_for_taxid(tid: str):\n",
        "    if tid in _tax_cache:\n",
        "        return _tax_cache[tid]\n",
        "    try:\n",
        "        node = fetch_taxnode_by_id(tid)\n",
        "        ph = lineage_rank(node, \"phylum\") or \"NA\"\n",
        "        nm = node.get(\"ScientificName\", \"Unknown\")\n",
        "        _tax_cache[tid] = (ph, nm)\n",
        "        return ph, nm\n",
        "    except Exception as e:\n",
        "        sys.stderr.write(f\"[warn] taxid {tid} lookup failed: {e}\\n\")\n",
        "        _tax_cache[tid] = (\"NA\", \"Unknown\")\n",
        "        return \"NA\", \"Unknown\"\n",
        "\n",
        "def taxid_from_accession(acc: str):\n",
        "    \"\"\"Resolve protein accession â†’ TaxID via esummary (cached).\"\"\"\n",
        "    if not acc: return None\n",
        "    if acc in _acc2tid: return _acc2tid[acc]\n",
        "    try:\n",
        "        h = Entrez.esummary(db=\"protein\", id=acc, retmode=\"xml\")\n",
        "        s = Entrez.read(h); h.close()\n",
        "        # esummary returns a list of DocSums; TaxId often in 'TaxId' or 'TaxID'\n",
        "        if s and \"DocSum\" in s[0]:\n",
        "            for item in s[0][\"DocSum\"][\"Item\"]:\n",
        "                if item.attributes.get(\"Name\") in (\"TaxId\",\"TaxID\"):\n",
        "                    tid = str(item.value)\n",
        "                    _acc2tid[acc] = tid\n",
        "                    return tid\n",
        "        # Alternate path: esummary REST sometimes packs fields differently; try generic keys\n",
        "        if s and isinstance(s[0], dict):\n",
        "            tid = s[0].get(\"TaxId\") or s[0].get(\"TaxID\")\n",
        "            if tid:\n",
        "                tid = str(tid)\n",
        "                _acc2tid[acc] = tid\n",
        "                return tid\n",
        "    except Exception as e:\n",
        "        sys.stderr.write(f\"[warn] esummary failed for {acc}: {e}\\n\")\n",
        "    _acc2tid[acc] = None\n",
        "    return None\n",
        "\n",
        "def seq_hash(seq):\n",
        "    return hashlib.sha256(seq.encode(\"ascii\", \"ignore\")).hexdigest()\n",
        "\n",
        "# ====== collect records from all inputs; deduplicate ======\n",
        "all_records = []\n",
        "for p in IN_FASTAS:\n",
        "    if not p.exists(): continue\n",
        "    all_records.extend(list(SeqIO.parse(str(p), \"fasta\")))\n",
        "\n",
        "if not all_records:\n",
        "    raise ValueError(\"No sequences found in input FASTAs.\")\n",
        "\n",
        "# Deduplicate primarily by accession if present, else by sequence hash\n",
        "seen_keys = set()\n",
        "kept = []\n",
        "for r in all_records:\n",
        "    header = r.description or r.id\n",
        "    acc = first_accession_token(header)\n",
        "    key = (\"acc\", acc) if acc else (\"seq\", seq_hash(str(r.seq)))\n",
        "    if key in seen_keys:\n",
        "        continue\n",
        "    seen_keys.add(key)\n",
        "    kept.append(r)\n",
        "\n",
        "print(f\"ðŸ“¦ Loaded {len(all_records)} records from {len(IN_FASTAS)} file(s); kept {len(kept)} after dedup.\")\n",
        "\n",
        "# ====== resolve TaxID for each kept record ======\n",
        "uniq_taxids = set()\n",
        "for r in kept:\n",
        "    header = r.description or r.id\n",
        "    tid = extract_taxid_from_header(header)\n",
        "    if not tid:\n",
        "        acc = first_accession_token(header)\n",
        "        tid = taxid_from_accession(acc)\n",
        "        # small pause every few lookups\n",
        "        time.sleep(0.1)\n",
        "    r.annotations[\"taxid\"] = tid\n",
        "    if tid: uniq_taxids.add(tid)\n",
        "\n",
        "print(f\"ðŸ”Ž Resolved TaxID for {sum(1 for r in kept if r.annotations['taxid'])} / {len(kept)} records.\")\n",
        "\n",
        "# Pre-fetch phylum/name for all TaxIDs we have\n",
        "for i, tid in enumerate(sorted(uniq_taxids, key=int)):\n",
        "    phylum_and_name_for_taxid(tid)\n",
        "    if (i+1) % 5 == 0:\n",
        "        time.sleep(0.2)\n",
        "\n",
        "# ====== write output with requested headers ======\n",
        "written = 0\n",
        "with open(OUT_FASTA, \"w\") as out_fa, open(MAP_CSV, \"w\", newline=\"\") as mapf:\n",
        "    w = csv.writer(mapf)\n",
        "    w.writerow([\"accession\",\"taxid\",\"phylum\",\"scientific_name\"])\n",
        "    for r in kept:\n",
        "        header = r.description or r.id\n",
        "        acc = first_accession_token(header) or \"\"\n",
        "        tid = r.annotations.get(\"taxid\")\n",
        "        if tid:\n",
        "            ph, nm = phylum_and_name_for_taxid(tid)\n",
        "        else:\n",
        "            ph, nm = \"NA\", \"Unknown\"\n",
        "        # EXACT requested format:\n",
        "        r.id = f\"{ph} {nm}\"\n",
        "        r.name = \"\"\n",
        "        r.description = \"\"\n",
        "        # If you need unique headers, uncomment the next line:\n",
        "        # r.id = f\"{ph} {nm} | acc:{acc or 'NA'}\"\n",
        "        SeqIO.write(r, out_fa, \"fasta\")\n",
        "        w.writerow([acc, tid or \"\", ph, nm])\n",
        "        written += 1\n",
        "\n",
        "print(f\"âœ… Wrote {written} sequences to: {OUT_FASTA}\")\n",
        "print(f\"ðŸ—º  Wrote TaxIDâ†’(phylum,name) map: {MAP_CSV}\")"
      ],
      "metadata": {
        "id": "Rn5arD3kTt25",
        "outputId": "789d1201-a827-4799-adc8-d1c1f35c4f3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Loaded 77 records from 1 file(s); kept 76 after dedup.\n",
            "ðŸ”Ž Resolved TaxID for 76 / 76 records.\n",
            "âœ… Wrote 76 sequences to: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/all_hits.phylum_name.fasta\n",
            "ðŸ—º  Wrote TaxIDâ†’(phylum,name) map: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/all_hits.phylum_name.taxid_map.csv\n"
          ]
        }
      ]
    }
  ]
}