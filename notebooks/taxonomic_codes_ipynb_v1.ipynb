{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJD/rCsfYgBqt80E6oYdIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobBurnap/Bioinformatics-MICR4203-MICR5203/blob/main/notebooks/taxonomic_codes_ipynb_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Developing a taxonomic table for taxonomic codes for a diverse set of sequences**"
      ],
      "metadata": {
        "id": "afZn240E8-8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "BIOINFO4/5203 ‚Äî Colab Exercise Template\n",
        "\n",
        "Use this template for every weekly exercise. It standardizes setup, data paths, and the final summary so grading in Canvas is quick.\n",
        "\n",
        "Workflow\n",
        "\n",
        "    Click the \"Open in Colab\" link in Canvas (points to this notebook in GitHub).\n",
        "    Run Setup cells (installs and mounts Google Drive).\n",
        "    Run the Exercise cells (edit as instructed for each lecture).\n",
        "    Verify the Results Summary prints the values requested by Canvas.\n",
        "    File ‚Üí Print ‚Üí Save as PDF and upload .ipynb + PDF to Canvas.\n",
        "\n",
        "    Instructor note (delete in student copy if desired):\n",
        "\n",
        "        Place datasets for this lecture at: Drive ‚Üí BIOINFO4-5203-F25 ‚Üí Data ‚Üí Lxx_topic\n",
        "        Update the constants in Config below: COURSE_DIR, LECTURE_CODE (e.g., L05), and TOPIC.\n",
        "        For heavy jobs (trees, assemblies), provide the PETE output files in the same Data folder so students can analyze them here if the queue is busy.\n",
        "\n"
      ],
      "metadata": {
        "id": "frmCc0lAQcYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto‚Äësetup + course folder (uses your Teaching path)**"
      ],
      "metadata": {
        "id": "bw0BsPOEGVL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A. Mount Google Drive, Import Coding Libraries Necessary for Running Subsequent Code"
      ],
      "metadata": {
        "id": "6JW7liKMC3H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install FIRST, then import\n",
        "%pip install -q biopython       # Install the Biopython package quietly (-q suppresses most output) so we can work with biological sequence files\n",
        "\n",
        "from google.colab import drive  # Import the module that lets Colab interact with Google Drive\n",
        "drive.mount('/content/drive')   # Mount your Google Drive so it appears in Colab's file system under /content/drive\n",
        "\n",
        "import os, pandas as pd          # Import 'os' for file/directory operations, and pandas for working with data tables\n",
        "from Bio import SeqIO            # Import SeqIO from Biopython for reading/writing biological sequence files (FASTA, GenBank, etc.)\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib's plotting library to create figures and graphs\n",
        "\n",
        "print(\"‚úÖ Dependencies installed & Drive mounted.\")\n"
      ],
      "metadata": {
        "id": "N1NrV2YaGfZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08480d0-c264-4034-aecb-a3994ce6775d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/3.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "‚úÖ Dependencies installed & Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## B. Course folders: Define the course folders for places to load data to be processed and output to be saved\n",
        "\n",
        "Edit only `LECTURE_CODE` and `TOPIC` if needed. All inputs will live in `Data/LECTURE_TOPIC` and outputs in `Outputs/LECTURE_TOPIC`."
      ],
      "metadata": {
        "id": "j7uJVzrKC-8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Define file sturcture"
      ],
      "metadata": {
        "id": "onPHWY4dGku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Course folder config (customize LECTURE_CODE/TOPIC only) ---\n",
        "COURSE_DIR   = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "LECTURE_CODE = \"Taxonomy\"            # change per week (e.g., L02, L03, ...)\n",
        "TOPIC        = \"Template\"    # short slug for the exercise\n",
        "\n",
        "# Derived paths (do not change)\n",
        "DATA_DIR   = f\"{COURSE_DIR}/Data/{LECTURE_CODE}_{TOPIC}\"\n",
        "OUTPUT_DIR = f\"{COURSE_DIR}/Outputs/{LECTURE_CODE}_{TOPIC}\"\n",
        "\n",
        "# Create folder structure if missing\n",
        "for p in [f\"{COURSE_DIR}/Data\", f\"{COURSE_DIR}/Outputs\", f\"{COURSE_DIR}/Notebooks\", DATA_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ COURSE_DIR :\", COURSE_DIR)\n",
        "print(\"üìÅ DATA_DIR   :\", DATA_DIR)\n",
        "print(\"üìÅ OUTPUT_DIR :\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1wyBalSGox_",
        "outputId": "bdfac62d-05a6-460f-e9e0-0b4d39414e24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ COURSE_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\n",
            "üìÅ DATA_DIR   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/Taxonomy_Template\n",
            "üìÅ OUTPUT_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Take file name \"taxonomic_cmmon_names\" and find taxonomic ids"
      ],
      "metadata": {
        "id": "H1Qe2LFAFz5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Parse FASTA ‚Üí CSV, then plot a quick figure ‚Üí PNG in Outputs/**"
      ],
      "metadata": {
        "id": "ccijET3tGsBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Read 'taxonomic_common_names' from DATA_DIR, resolve to TaxIDs (incl. common names), merge & write outputs ---\n",
        "from Bio import Entrez\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import csv, io, re, time, sys, os\n",
        "\n",
        "# REQUIRED\n",
        "Entrez.email = \"you@university.edu\"   # <-- set your email here\n",
        "\n",
        "# Expect DATA_DIR and OUTPUT_DIR from your course scaffold\n",
        "assert 'DATA_DIR' in globals() and 'OUTPUT_DIR' in globals(), \"Run your course folder setup cell first.\"\n",
        "DATA_DIR = Path(DATA_DIR)\n",
        "OUT      = Path(OUTPUT_DIR)\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Locate the names file (exact name requested) ----\n",
        "# Accept either no extension, .txt, .csv, or .tsv\n",
        "CANDIDATES = [\n",
        "    DATA_DIR / \"taxonomic_common_names\",\n",
        "    DATA_DIR / \"taxonomic_common_names.txt\",\n",
        "    DATA_DIR / \"taxonomic_common_names.csv\",\n",
        "    DATA_DIR / \"taxonomic_c0mmon_names.tsv\",\n",
        "]\n",
        "NAMES_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
        "if not NAMES_PATH:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find 'taxonomic_cmmon_names' in {DATA_DIR}. \"\n",
        "        \"Create it as a plain text/CSV/TSV file (one name per line; first column if table).\"\n",
        "    )\n",
        "print(f\"üìÑ Names file: {NAMES_PATH}\")\n",
        "\n",
        "# ---- Load names (plain text OR first column of CSV/TSV). Lines starting with # are ignored. ----\n",
        "def load_names(path: Path):\n",
        "    text = path.read_text(errors=\"ignore\")\n",
        "    # Heuristic: treat as table if we see a comma or tab in first few lines\n",
        "    head = \"\\n\".join(text.splitlines()[:5])\n",
        "    is_table = (\",\" in head) or (\"\\t\" in head)\n",
        "    names = []\n",
        "    if is_table:\n",
        "        # Try TSV first if tabs present, else CSV\n",
        "        sep = \"\\t\" if \"\\t\" in head else \",\"\n",
        "        df = pd.read_csv(io.StringIO(text), sep=sep, comment=\"#\", header=None)\n",
        "        # take first column\n",
        "        col = df.columns[0]\n",
        "        names = [str(x).strip() for x in df[col].tolist() if str(x).strip()]\n",
        "    else:\n",
        "        for line in text.splitlines():\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            names.append(line)\n",
        "    return names\n",
        "\n",
        "raw_names = load_names(NAMES_PATH)\n",
        "if not raw_names:\n",
        "    raise ValueError(f\"No names found in {NAMES_PATH} (file empty or only comments).\")\n",
        "print(f\"üìù Loaded {len(raw_names)} name(s)\")\n",
        "\n",
        "# ---- Resolve name -> TaxID (try scientific first, then common name, then loose) ----\n",
        "def tax_lookup(name: str):\n",
        "    \"\"\"\n",
        "    Resolve 'name' (scientific or common) -> (taxid, canonical, rank, match_type, status)\n",
        "    match_type: SCIN / COMMON / LOOSE\n",
        "    status: 'ok' or reason\n",
        "    \"\"\"\n",
        "    name = name.strip()\n",
        "    # If user accidentally included a TaxID, accept it directly\n",
        "    if re.fullmatch(r\"\\d+\", name):\n",
        "        try:\n",
        "            tid = int(name)\n",
        "            h2 = Entrez.efetch(db=\"taxonomy\", id=str(tid), retmode=\"xml\"); rec = Entrez.read(h2); h2.close()\n",
        "            node = rec[0]\n",
        "            return tid, node.get(\"ScientificName\",\"\"), node.get(\"Rank\",\"\"), \"TAXID\", \"ok\"\n",
        "        except Exception as e:\n",
        "            return \"\", \"\", \"\", \"TAXID\", f\"efetch_failed:{e}\"\n",
        "\n",
        "    queries = [\n",
        "        (f\"\\\"{name}\\\"[SCIN]\", \"SCIN\"),               # strict scientific name\n",
        "        (f\"\\\"{name}\\\"[Common Name]\", \"COMMON\"),     # strict common name\n",
        "        (name, \"LOOSE\"),                            # loose text search (all-name fields)\n",
        "    ]\n",
        "    tried = set()\n",
        "    for q, tag in queries:\n",
        "        if q in tried:\n",
        "            continue\n",
        "        tried.add(q)\n",
        "        try:\n",
        "            h = Entrez.esearch(db=\"taxonomy\", term=q, retmode=\"xml\"); r = Entrez.read(h); h.close()\n",
        "            if int(r[\"Count\"]) == 0:\n",
        "                continue\n",
        "            tid = r[\"IdList\"][0]\n",
        "            h2 = Entrez.efetch(db=\"taxonomy\", id=tid, retmode=\"xml\"); rec = Entrez.read(h2); h2.close()\n",
        "            node = rec[0]\n",
        "            canon = node.get(\"ScientificName\",\"\")\n",
        "            rank  = node.get(\"Rank\",\"\")\n",
        "            return int(tid), canon, rank, tag, \"ok\"\n",
        "        except Exception as e:\n",
        "            sys.stderr.write(f\"[warn] lookup '{name}' via {tag}: {e}\\n\")\n",
        "            continue\n",
        "    return \"\", \"\", \"\", \"NA\", \"no_match\"\n",
        "\n",
        "rows = []\n",
        "seen = set()\n",
        "for nm in raw_names:\n",
        "    nm = nm.strip()\n",
        "    if not nm or nm.lower() in seen:\n",
        "        continue\n",
        "    seen.add(nm.lower())\n",
        "    tid, canon, rank, tag, status = tax_lookup(nm)\n",
        "    rows.append({\n",
        "        \"input_name\": nm,\n",
        "        \"taxid\": tid,\n",
        "        \"canonical_name\": canon,\n",
        "        \"rank\": rank,\n",
        "        \"match_type\": tag,\n",
        "        \"status\": status\n",
        "    })\n",
        "    time.sleep(0.25)  # be polite to NCBI\n",
        "\n",
        "df_new = pd.DataFrame(rows).sort_values([\"status\",\"canonical_name\",\"input_name\"], na_position=\"last\")\n",
        "map_path = OUT / \"augmented_taxon_map.csv\"\n",
        "df_new.to_csv(map_path, index=False)\n",
        "print(f\"üó∫  Wrote map: {map_path}\")\n",
        "\n",
        "# ---- Merge with existing TaxID lists if present ----\n",
        "existing_taxids = set()\n",
        "for p in [OUT/\"taxids.txt\", OUT/\"taxids-augmented.txt\", DATA_DIR/\"taxids.txt\"]:\n",
        "    if p.exists():\n",
        "        existing_taxids |= {t.strip() for t in p.read_text().splitlines() if t.strip().isdigit()}\n",
        "\n",
        "new_taxids = {str(t) for t in df_new[\"taxid\"].tolist() if str(t).isdigit()}\n",
        "all_taxids = sorted(existing_taxids | new_taxids, key=lambda x: int(x))\n",
        "\n",
        "# ---- Write merged outputs ----\n",
        "(OUT / \"taxids-augmented.txt\").write_text(\"\\n\".join(all_taxids))\n",
        "eq_exp = \" OR \".join([f\"txid{t}[ORGN]\" for t in all_taxids])  # [ORGN] is robust in BLAST\n",
        "(OUT / \"entrez_query_exp_aug.txt\").write_text(eq_exp)\n",
        "\n",
        "# ---- Report unresolved names (if any) ----\n",
        "unresolved = df_new[df_new[\"status\"] != \"ok\"]\n",
        "if not unresolved.empty:\n",
        "    bad_list = \", \".join(unresolved[\"input_name\"].tolist()[:12])\n",
        "    print(f\"‚ö†Ô∏è Unresolved names: {len(unresolved)} (showing a few): {bad_list}\")\n",
        "\n",
        "print(f\"‚úÖ Resolved {len(new_taxids)} new TaxIDs. Total in merged list: {len(all_taxids)}\")\n",
        "print(\"üß¨ TaxIDs :\", OUT / 'taxids-augmented.txt')\n",
        "print(\"üîé ENTREQ :\", OUT / 'entrez_query_exp_aug.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y14FsfkKF4f5",
        "outputId": "e0e18159-fd8c-4e50-fdc7-e0fcdd8496e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Names file: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/Taxonomy_Template/taxonomic_common_names.txt\n",
            "üìù Loaded 127 name(s)\n",
            "üó∫  Wrote map: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/augmented_taxon_map.csv\n",
            "‚ö†Ô∏è Unresolved names: 1 (showing a few): Candidatus Syntrophoarchaeum butanivorans\n",
            "‚úÖ Resolved 123 new TaxIDs. Total in merged list: 123\n",
            "üß¨ TaxIDs : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/taxids-augmented.txt\n",
            "üîé ENTREQ : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/Taxonomy_Template/entrez_query_exp_aug.txt\n"
          ]
        }
      ]
    }
  ]
}