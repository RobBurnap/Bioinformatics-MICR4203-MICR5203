{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobBurnap/Bioinformatics-MICR4203-MICR5203/blob/main/notebooks/Species_Tree_Diversity_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275544ee",
      "metadata": {
        "id": "275544ee"
      },
      "source": [
        "\n",
        "# BIOINFO4/5203 —\n",
        "species diveristy:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A. Mount Google Drive, Import Coding Libraries Necessary for Running Subsequent Code"
      ],
      "metadata": {
        "id": "fU6YPxFnnl1Y"
      },
      "id": "fU6YPxFnnl1Y"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "269c6b87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "269c6b87",
        "outputId": "d968bcc1-4ef2-487f-a941-3a351e3082fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Dependencies installed & Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install FIRST, then import\n",
        "%pip install -q biopython       # Install the Biopython package quietly (-q suppresses most output) so we can work with biological sequence files\n",
        "\n",
        "from google.colab import drive  # Import the module that lets Colab interact with Google Drive\n",
        "drive.mount('/content/drive')   # Mount your Google Drive so it appears in Colab's file system under /content/drive\n",
        "\n",
        "import os, pandas as pd          # Import 'os' for file/directory operations, and pandas for working with data tables\n",
        "from Bio import SeqIO            # Import SeqIO from Biopython for reading/writing biological sequence files (FASTA, GenBank, etc.)\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib's plotting library to create figures and graphs\n",
        "\n",
        "print(\"✅ Dependencies installed & Drive mounted.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fa44d1",
      "metadata": {
        "id": "42fa44d1"
      },
      "source": [
        "\n",
        "## B. Course folders: Define the course folders for places to load data to be processed and output to be saved\n",
        "\n",
        "Edit only `LECTURE_CODE` and `TOPIC` if needed. All inputs will live in `Data/LECTURE_TOPIC` and outputs in `Outputs/LECTURE_TOPIC`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "66b0e9d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66b0e9d7",
        "outputId": "3032db98-47b8-4c8f-a439-186799528dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 COURSE_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\n",
            "📁 DATA_DIR   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\n",
            "📁 OUTPUT_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L0-species_diversity\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Course folder config (customize LECTURE_CODE/TOPIC only) ---\n",
        "COURSE_DIR   = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "LECTURE_CODE = \"L0-species\"            # change per week (e.g., L02, L03, ...)\n",
        "TOPIC        = \"diversity\"    # short slug for the exercise\n",
        "\n",
        "# Derived paths (do not change)\n",
        "DATA_DIR   = f\"{COURSE_DIR}/Data/{LECTURE_CODE}_{TOPIC}\"\n",
        "OUTPUT_DIR = f\"{COURSE_DIR}/Outputs/{LECTURE_CODE}_{TOPIC}\"\n",
        "\n",
        "# Create folder structure if missing\n",
        "for p in [f\"{COURSE_DIR}/Data\", f\"{COURSE_DIR}/Outputs\", f\"{COURSE_DIR}/Notebooks\", DATA_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"📁 COURSE_DIR :\", COURSE_DIR)\n",
        "print(\"📁 DATA_DIR   :\", DATA_DIR)\n",
        "print(\"📁 OUTPUT_DIR :\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C.\n",
        "\n"
      ],
      "metadata": {
        "id": "ITJ9NS_HlbIU"
      },
      "id": "ITJ9NS_HlbIU"
    },
    {
      "cell_type": "markdown",
      "source": [
        " multi-FASTA of top hits per TaxID (one or more sequences per taxon). The cell below:\n",
        "\t•\tuses your existing folders (Data/L0-species_diversity for input; writes to the same folder unless OUTPUT_DIR is already set),\n",
        "\t•\treads query_proteins.fasta and taxids.txt,\n",
        "\t•\tfor each TaxID, runs a separate BLAST restricted to that taxon (txid####[ORGN]), so we can attribute hits unambiguously,\n",
        "\t•\tgrabs the top N accessions from each BLAST,\n",
        "\t•\tfetches their protein FASTA sequences,\n",
        "\t•\twrites:\n",
        "\t•\tper_taxid_top_hits.fasta (all sequences, grouped by taxid in headers),\n",
        "\t•\tper_taxid_hits.tsv (who came from which taxid, evalue, %id, etc.),\n",
        "\t•\toptional one FASTA per taxid (toggle with WRITE_SPLIT_FASTA)."
      ],
      "metadata": {
        "id": "ADIsRNvmtr3K"
      },
      "id": "ADIsRNvmtr3K"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctgn3Y6EpkXa"
      },
      "id": "ctgn3Y6EpkXa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLAST (per TaxID) -> collect top protein hits -> write multi-FASTA + TSV (fixed mode detection) ---\n",
        "from Bio import Entrez, SeqIO\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from pathlib import Path\n",
        "import io, csv, re, time, sys\n",
        "\n",
        "# ==== required ====\n",
        "Entrez.email = \"you@university.edu\"   # <-- set your email\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"  # optional, faster/higher limits\n",
        "\n",
        "# ==== knobs you can tweak ====\n",
        "TOP_HITS_PER_TAXID = 2          # how many sequences to keep per taxid\n",
        "EVALUE              = 1e-5      # keep stringent\n",
        "HITLIST_SIZE        = max(50, TOP_HITS_PER_TAXID*10)\n",
        "WRITE_SPLIT_FASTA   = False\n",
        "SLEEP_BETWEEN_CALLS = 0.3\n",
        "\n",
        "# Optional hard override: 'auto' | 'blastp' | 'blastx'\n",
        "FORCE_MODE = 'auto'   # set to 'blastp' if your query file is protein but misdetected\n",
        "\n",
        "# ==== paths (use your course vars if present) ====\n",
        "if 'DATA_DIR' in globals():\n",
        "    DATA_DIR = Path(DATA_DIR)\n",
        "else:\n",
        "    DATA_DIR = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\")\n",
        "\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "else:\n",
        "    OUTPUT_DIR = DATA_DIR\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Files (per your folder)\n",
        "PROT_FASTA = DATA_DIR / \"query_proteins.fasta\"\n",
        "NUC_FASTA  = DATA_DIR / \"query.fasta\"\n",
        "TAXIDS_TXT = DATA_DIR / \"taxids.txt\"\n",
        "\n",
        "# ==== helpers: accession parsing & normalization ====\n",
        "def extract_accession(hit_id, hit_def, accession_attr):\n",
        "    if accession_attr:\n",
        "        return accession_attr.strip()\n",
        "    for field in (hit_id, hit_def):\n",
        "        m = re.search(r\"([A-Z]{1,5}_?\\d+(?:\\.\\d+)?)\", field or \"\")\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return (hit_id or \"unknown\").strip()\n",
        "\n",
        "def _norm_keys(acc_token: str):\n",
        "    acc_token = acc_token.strip()\n",
        "    base = acc_token.split(\".\", 1)[0]\n",
        "    if \".\" in acc_token:\n",
        "        return {acc_token, base}\n",
        "    else:\n",
        "        return {acc_token, f\"{base}.1\"}\n",
        "\n",
        "def fetch_protein_fasta(accessions):\n",
        "    out = {}\n",
        "    batch = list({a for a in accessions if a and a != \"unknown\"})\n",
        "    while batch:\n",
        "        chunk = batch[:50]; batch = batch[50:]\n",
        "        try:\n",
        "            h = Entrez.efetch(db=\"protein\", id=\",\".join(chunk), rettype=\"fasta\", retmode=\"text\")\n",
        "            txt = h.read(); h.close()\n",
        "            parts = [t for t in txt.strip().split(\">\") if t]\n",
        "            for rec_txt in parts:\n",
        "                header, *seq_lines = rec_txt.splitlines()\n",
        "                token = header.split()[0]  # e.g., XOB97566.1 or WP_12345.1\n",
        "                fasta_txt = \">\" + header + \"\\n\" + \"\\n\".join(seq_lines) + \"\\n\"\n",
        "                for k in _norm_keys(token):\n",
        "                    out[k] = fasta_txt\n",
        "        except Exception as e:\n",
        "            sys.stderr.write(f\"[warn] efetch failed for {chunk}: {e}\\n\")\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "    return out\n",
        "\n",
        "# ==== robust query loader/detector ====\n",
        "DNA_ALPHABET = set(\"ACGTNUWSMKRYBDHV-\")  # IUPAC DNA (upper)\n",
        "def detect_is_protein(seq_upper: str) -> bool:\n",
        "    letters = [c for c in seq_upper if c.isalpha() or c == '*']\n",
        "    # If there's ANY character outside DNA alphabet (e.g., E, F, L, Q, Z, J, O), it's protein.\n",
        "    return any((c not in DNA_ALPHABET) for c in letters)\n",
        "\n",
        "def load_query():\n",
        "    # Prefer explicit protein file if present\n",
        "    if PROT_FASTA.exists():\n",
        "        rec = next(SeqIO.parse(str(PROT_FASTA), \"fasta\"))\n",
        "        seq = str(rec.seq).upper()\n",
        "        is_prot = detect_is_protein(seq)\n",
        "        decided = 'blastp' if is_prot else 'blastx'\n",
        "        if FORCE_MODE in ('blastp','blastx'):\n",
        "            decided = FORCE_MODE\n",
        "        return decided, seq.replace(\"*\",\"\"), f\"{PROT_FASTA.name}:{rec.id}\"\n",
        "\n",
        "    # Fallback to query.fasta\n",
        "    if NUC_FASTA.exists():\n",
        "        rec = next(SeqIO.parse(str(NUC_FASTA), \"fasta\"))\n",
        "        seq = str(rec.seq).upper()\n",
        "        is_prot = detect_is_protein(seq)\n",
        "        decided = 'blastp' if is_prot else 'blastx'\n",
        "        if FORCE_MODE in ('blastp','blastx'):\n",
        "            decided = FORCE_MODE\n",
        "        # Warn if filename suggests nucleotide but content looks protein\n",
        "        if decided == 'blastp' and NUC_FASTA.name == \"query.fasta\":\n",
        "            print(\"⚠️  Detected protein sequence in 'query.fasta'; using BLASTP. \"\n",
        "                  \"If this is actually nucleotide, set FORCE_MODE='blastx'.\")\n",
        "        return decided, seq.replace(\"*\",\"\"), f\"{NUC_FASTA.name}:{rec.id}\"\n",
        "\n",
        "    raise FileNotFoundError(f\"Could not find {PROT_FASTA} or {NUC_FASTA}\")\n",
        "\n",
        "mode, query_seq, query_label = load_query()\n",
        "print(f\"📄 Query source: {query_label}\")\n",
        "print(f\"🧪 Mode chosen: {mode.upper()} vs nr  | length={len(query_seq)}\")\n",
        "\n",
        "# ==== load TaxIDs ====\n",
        "if not TAXIDS_TXT.exists():\n",
        "    raise FileNotFoundError(f\"taxids.txt not found at {TAXIDS_TXT}\")\n",
        "taxids = [t.strip() for t in TAXIDS_TXT.read_text().splitlines() if t.strip().isdigit()]\n",
        "if not taxids:\n",
        "    raise ValueError(\"taxids.txt is empty or contains no numeric TaxIDs.\")\n",
        "print(f\"🧬 Loaded {len(taxids)} TaxIDs\")\n",
        "\n",
        "# ==== BLAST per-taxid ====\n",
        "def run_single_blast(seq, taxid, program=\"blastp\", evalue=EVALUE, hitlist=HITLIST_SIZE):\n",
        "    q = f\"txid{taxid}[ORGN]\"\n",
        "    preview = q if len(q) < 160 else (q[:157] + \" …\")\n",
        "    print(f\"⏳ {program.upper()} vs nr | taxid={taxid} | E={evalue} | hits={hitlist}\\n   ENTREZ_QUERY: {preview}\")\n",
        "    h = NCBIWWW.qblast(program=program, database=\"nr\", sequence=seq,\n",
        "                       expect=evalue, entrez_query=q,\n",
        "                       hitlist_size=hitlist, descriptions=hitlist, alignments=hitlist)\n",
        "    xml = h.read(); h.close()\n",
        "    rec = NCBIXML.read(io.StringIO(xml))\n",
        "    return rec, xml\n",
        "\n",
        "# ==== main loop -> XML + top hits -> fetch FASTA ====\n",
        "all_rows = []\n",
        "per_taxid_fastas = {}   # taxid -> list of FASTA strings\n",
        "xml_paths = []\n",
        "\n",
        "for i, tid in enumerate(taxids, 1):\n",
        "    try:\n",
        "        record, xml = run_single_blast(query_seq, tid, program=(\"blastp\" if mode==\"blastp\" else \"blastx\"))\n",
        "        xml_file = OUTPUT_DIR / f\"{mode}_nr_taxid{tid}.xml\"\n",
        "        xml_file.write_text(xml); xml_paths.append(xml_file)\n",
        "\n",
        "        if not record.alignments:\n",
        "            print(f\"— No hits for taxid {tid}\")\n",
        "            continue\n",
        "\n",
        "        # Collect best HSP per alignment; rank by (evalue, -pct_id)\n",
        "        hsps = []\n",
        "        for aln in record.alignments:\n",
        "            best = sorted(aln.hsps, key=lambda h: (h.expect, -h.identities))[0]\n",
        "            acc = extract_accession(aln.hit_id, aln.hit_def, getattr(aln, \"accession\", None))\n",
        "            pct_id = 100.0 * best.identities / best.align_length if best.align_length else 0.0\n",
        "            hsps.append((aln, best, acc, pct_id))\n",
        "        hsps.sort(key=lambda t: (t[1].expect, -t[3]))\n",
        "        keep = hsps[:TOP_HITS_PER_TAXID]\n",
        "\n",
        "        # Fetch FASTAs for these accessions (store under both base/version keys)\n",
        "        accs = [acc for _,_,acc,_ in keep]\n",
        "        acc_to_fa = fetch_protein_fasta(accs)\n",
        "\n",
        "        # Store chosen sequences\n",
        "        per_taxid_fastas.setdefault(tid, [])\n",
        "        kept_now = 0\n",
        "        for aln, best, acc, pct in keep:\n",
        "            fa = None\n",
        "            for k in _norm_keys(acc):\n",
        "                fa = acc_to_fa.get(k)\n",
        "                if fa: break\n",
        "            if not fa:\n",
        "                sys.stderr.write(f\"[miss] No FASTA for {acc} (taxid {tid})\\n\")\n",
        "                continue\n",
        "\n",
        "            # Prepend taxid info to header\n",
        "            lines = fa.strip().splitlines()\n",
        "            header = lines[0][1:]  # drop '>'\n",
        "            seq = \"\\n\".join(lines[1:])\n",
        "            new_header = f\">taxid:{tid}|acc:{acc}|e:{best.expect:.2e}|pid:{pct:.2f}|len:{best.align_length} {header}\"\n",
        "            per_taxid_fastas[tid].append(new_header + \"\\n\" + seq + \"\\n\"); kept_now += 1\n",
        "\n",
        "            all_rows.append([\n",
        "                tid, acc, aln.title, aln.length, best.expect,\n",
        "                best.identities, best.align_length, round(pct,2),\n",
        "                min(best.query_start,best.query_end), max(best.query_start,best.query_end),\n",
        "                min(best.sbjct_start,best.sbjct_end), max(best.sbjct_start,best.sbjct_end)\n",
        "            ])\n",
        "        print(f\"✅ taxid {tid}: kept {kept_now} sequences\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ taxid {tid} failed: {e}\")\n",
        "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "\n",
        "# ==== write combined multi-FASTA + per-taxid FASTAs ====\n",
        "multi_fa = OUTPUT_DIR / \"per_taxid_top_hits.fasta\"\n",
        "with open(multi_fa, \"w\") as fh:\n",
        "    for tid in taxids:\n",
        "        for fa in per_taxid_fastas.get(tid, []):\n",
        "            fh.write(fa)\n",
        "print(f\"💾 Multi-FASTA: {multi_fa}\")\n",
        "\n",
        "if WRITE_SPLIT_FASTA:\n",
        "    for tid, fas in per_taxid_fastas.items():\n",
        "        p = OUTPUT_DIR / f\"taxid_{tid}_top_hits.fasta\"\n",
        "        with open(p, \"w\") as fh:\n",
        "            for fa in fas: fh.write(fa)\n",
        "    print(\"💾 Also wrote per-taxid FASTAs\")\n",
        "\n",
        "# ==== write table ====\n",
        "tsv = OUTPUT_DIR / \"per_taxid_hits.tsv\"\n",
        "with open(tsv, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f, delimiter=\"\\t\")\n",
        "    w.writerow([\"taxid\",\"accession\",\"title\",\"subject_length\",\"evalue\",\"identities\",\"align_len\",\"pct_identity\",\"q_start\",\"q_end\",\"s_start\",\"s_end\"])\n",
        "    w.writerows(all_rows)\n",
        "print(f\"📑 Table: {tsv}\")\n",
        "\n",
        "print(f\"🗂 XML files saved: {len(xml_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSGXBMA5tc72",
        "outputId": "1fe4f4da-7afd-4a89-da30-9f1cf5b2a5ef"
      },
      "id": "tSGXBMA5tc72",
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Query source: query_proteins.fasta:cyt-c6\n",
            "🧪 Mode chosen: BLASTP vs nr  | length=111\n",
            "🧬 Loaded 123 TaxIDs\n",
            "⏳ BLASTP vs nr | taxid=271 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid271[ORGN]\n",
            "✅ taxid 271: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=277 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid277[ORGN]\n",
            "✅ taxid 277: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=562 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid562[ORGN]\n",
            "— No hits for taxid 562\n",
            "⏳ BLASTP vs nr | taxid=1148 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1148[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[miss] No FASTA for 9KRR_A (taxid 1148)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ taxid 1148: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=1179 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1179[ORGN]\n",
            "✅ taxid 1179: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=1423 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1423[ORGN]\n",
            "✅ taxid 1423: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=6204 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid6204[ORGN]\n",
            "— No hits for taxid 6204\n",
            "⏳ BLASTP vs nr | taxid=6205 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid6205[ORGN]\n",
            "— No hits for taxid 6205\n",
            "⏳ BLASTP vs nr | taxid=6207 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid6207[ORGN]\n",
            "— No hits for taxid 6207\n",
            "⏳ BLASTP vs nr | taxid=6210 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid6210[ORGN]\n",
            "— No hits for taxid 6210\n",
            "⏳ BLASTP vs nr | taxid=6211 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid6211[ORGN]\n",
            "— No hits for taxid 6211\n",
            "⏳ BLASTP vs nr | taxid=6645 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid6645[ORGN]\n",
            "— No hits for taxid 6645\n",
            "⏳ BLASTP vs nr | taxid=7010 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid7010[ORGN]\n",
            "— No hits for taxid 7010\n",
            "⏳ BLASTP vs nr | taxid=7217 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid7217[ORGN]\n",
            "— No hits for taxid 7217\n",
            "⏳ BLASTP vs nr | taxid=7653 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid7653[ORGN]\n",
            "— No hits for taxid 7653\n",
            "⏳ BLASTP vs nr | taxid=9407 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9407[ORGN]\n",
            "— No hits for taxid 9407\n",
            "⏳ BLASTP vs nr | taxid=9593 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9593[ORGN]\n",
            "— No hits for taxid 9593\n",
            "⏳ BLASTP vs nr | taxid=9597 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9597[ORGN]\n",
            "— No hits for taxid 9597\n",
            "⏳ BLASTP vs nr | taxid=9598 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9598[ORGN]\n",
            "— No hits for taxid 9598\n",
            "⏳ BLASTP vs nr | taxid=9601 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9601[ORGN]\n",
            "— No hits for taxid 9601\n",
            "⏳ BLASTP vs nr | taxid=9606 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9606[ORGN]\n",
            "— No hits for taxid 9606\n",
            "⏳ BLASTP vs nr | taxid=9739 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9739[ORGN]\n",
            "— No hits for taxid 9739\n",
            "⏳ BLASTP vs nr | taxid=9764 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9764[ORGN]\n",
            "— No hits for taxid 9764\n",
            "⏳ BLASTP vs nr | taxid=9785 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9785[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request AD78U83Z015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— No hits for taxid 9785\n",
            "⏳ BLASTP vs nr | taxid=9823 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9823[ORGN]\n",
            "— No hits for taxid 9823\n",
            "⏳ BLASTP vs nr | taxid=9913 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9913[ORGN]\n",
            "— No hits for taxid 9913\n",
            "⏳ BLASTP vs nr | taxid=9940 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid9940[ORGN]\n",
            "— No hits for taxid 9940\n",
            "⏳ BLASTP vs nr | taxid=10090 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid10090[ORGN]\n",
            "— No hits for taxid 10090\n",
            "⏳ BLASTP vs nr | taxid=10116 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid10116[ORGN]\n",
            "— No hits for taxid 10116\n",
            "⏳ BLASTP vs nr | taxid=32046 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid32046[ORGN]\n",
            "✅ taxid 32046: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=34597 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid34597[ORGN]\n",
            "— No hits for taxid 34597\n",
            "⏳ BLASTP vs nr | taxid=37636 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid37636[ORGN]\n",
            "✅ taxid 37636: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=39669 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid39669[ORGN]\n",
            "— No hits for taxid 39669\n",
            "⏳ BLASTP vs nr | taxid=45264 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid45264[ORGN]\n",
            "— No hits for taxid 45264\n",
            "⏳ BLASTP vs nr | taxid=47478 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid47478[ORGN]\n",
            "✅ taxid 47478: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=50339 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid50339[ORGN]\n",
            "— No hits for taxid 50339\n",
            "⏳ BLASTP vs nr | taxid=51298 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid51298[ORGN]\n",
            "— No hits for taxid 51298\n",
            "⏳ BLASTP vs nr | taxid=52022 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid52022[ORGN]\n",
            "✅ taxid 52022: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=53468 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid53468[ORGN]\n",
            "— No hits for taxid 53468\n",
            "⏳ BLASTP vs nr | taxid=56956 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid56956[ORGN]\n",
            "✅ taxid 56956: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=60517 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid60517[ORGN]\n",
            "— No hits for taxid 60517\n",
            "⏳ BLASTP vs nr | taxid=63221 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid63221[ORGN]\n",
            "— No hits for taxid 63221\n",
            "⏳ BLASTP vs nr | taxid=68909 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid68909[ORGN]\n",
            "✅ taxid 68909: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=85433 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid85433[ORGN]\n",
            "— No hits for taxid 85433\n",
            "⏳ BLASTP vs nr | taxid=94130 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid94130[ORGN]\n",
            "— No hits for taxid 94130\n",
            "⏳ BLASTP vs nr | taxid=102285 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid102285[ORGN]\n",
            "— No hits for taxid 102285\n",
            "⏳ BLASTP vs nr | taxid=105358 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid105358[ORGN]\n",
            "— No hits for taxid 105358\n",
            "⏳ BLASTP vs nr | taxid=112137 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid112137[ORGN]\n",
            "— No hits for taxid 112137\n",
            "⏳ BLASTP vs nr | taxid=115547 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid115547[ORGN]\n",
            "— No hits for taxid 115547\n",
            "⏳ BLASTP vs nr | taxid=159855 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid159855[ORGN]\n",
            "— No hits for taxid 159855\n",
            "⏳ BLASTP vs nr | taxid=172827 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid172827[ORGN]\n",
            "✅ taxid 172827: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=186192 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid186192[ORGN]\n",
            "— No hits for taxid 186192\n",
            "⏳ BLASTP vs nr | taxid=187137 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid187137[ORGN]\n",
            "— No hits for taxid 187137\n",
            "⏳ BLASTP vs nr | taxid=208447 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid208447[ORGN]\n",
            "— No hits for taxid 208447\n",
            "⏳ BLASTP vs nr | taxid=261391 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid261391[ORGN]\n",
            "— No hits for taxid 261391\n",
            "⏳ BLASTP vs nr | taxid=273116 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid273116[ORGN]\n",
            "— No hits for taxid 273116\n",
            "⏳ BLASTP vs nr | taxid=274614 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid274614[ORGN]\n",
            "— No hits for taxid 274614\n",
            "⏳ BLASTP vs nr | taxid=287848 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid287848[ORGN]\n",
            "✅ taxid 287848: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=334625 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid334625[ORGN]\n",
            "— No hits for taxid 334625\n",
            "⏳ BLASTP vs nr | taxid=411798 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid411798[ORGN]\n",
            "— No hits for taxid 411798\n",
            "⏳ BLASTP vs nr | taxid=457083 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid457083[ORGN]\n",
            "✅ taxid 457083: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=482827 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid482827[ORGN]\n",
            "✅ taxid 482827: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=540988 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid540988[ORGN]\n",
            "— No hits for taxid 540988\n",
            "⏳ BLASTP vs nr | taxid=543639 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid543639[ORGN]\n",
            "— No hits for taxid 543639\n",
            "⏳ BLASTP vs nr | taxid=667138 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid667138[ORGN]\n",
            "— No hits for taxid 667138\n",
            "⏳ BLASTP vs nr | taxid=869210 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid869210[ORGN]\n",
            "— No hits for taxid 869210\n",
            "⏳ BLASTP vs nr | taxid=1408545 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1408545[ORGN]\n",
            "✅ taxid 1408545: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=1416614 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1416614[ORGN]\n",
            "✅ taxid 1416614: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=1552121 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1552121[ORGN]\n",
            "✅ taxid 1552121: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=1644118 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1644118[ORGN]\n",
            "— No hits for taxid 1644118\n",
            "⏳ BLASTP vs nr | taxid=1673428 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1673428[ORGN]\n",
            "— No hits for taxid 1673428\n",
            "⏳ BLASTP vs nr | taxid=1737569 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1737569[ORGN]\n",
            "— No hits for taxid 1737569\n",
            "⏳ BLASTP vs nr | taxid=1752064 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1752064[ORGN]\n",
            "✅ taxid 1752064: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=1761908 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1761908[ORGN]\n",
            "✅ taxid 1761908: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=1874737 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1874737[ORGN]\n",
            "— No hits for taxid 1874737\n",
            "⏳ BLASTP vs nr | taxid=1904752 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1904752[ORGN]\n",
            "⚠️ taxid 1904752 failed: <urlopen error [Errno 99] Cannot assign requested address>\n",
            "⏳ BLASTP vs nr | taxid=1906666 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1906666[ORGN]\n",
            "— No hits for taxid 1906666\n",
            "⏳ BLASTP vs nr | taxid=1906667 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1906667[ORGN]\n",
            "— No hits for taxid 1906667\n",
            "⏳ BLASTP vs nr | taxid=1945595 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1945595[ORGN]\n",
            "— No hits for taxid 1945595\n",
            "⏳ BLASTP vs nr | taxid=1955249 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1955249[ORGN]\n",
            "✅ taxid 1955249: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=1973142 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1973142[ORGN]\n",
            "— No hits for taxid 1973142\n",
            "⏳ BLASTP vs nr | taxid=1982969 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1982969[ORGN]\n",
            "✅ taxid 1982969: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2005738 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2005738[ORGN]\n",
            "— No hits for taxid 2005738\n",
            "⏳ BLASTP vs nr | taxid=2026739 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2026739[ORGN]\n",
            "— No hits for taxid 2026739\n",
            "⏳ BLASTP vs nr | taxid=2026795 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2026795[ORGN]\n",
            "— No hits for taxid 2026795\n",
            "⏳ BLASTP vs nr | taxid=2026803 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2026803[ORGN]\n",
            "— No hits for taxid 2026803\n",
            "⏳ BLASTP vs nr | taxid=2032688 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2032688[ORGN]\n",
            "✅ taxid 2032688: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2078690 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2078690[ORGN]\n",
            "✅ taxid 2078690: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2591003 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2591003[ORGN]\n",
            "— No hits for taxid 2591003\n",
            "⏳ BLASTP vs nr | taxid=2608793 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2608793[ORGN]\n",
            "— No hits for taxid 2608793\n",
            "⏳ BLASTP vs nr | taxid=2664179 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2664179[ORGN]\n",
            "✅ taxid 2664179: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2714958 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2714958[ORGN]\n",
            "✅ taxid 2714958: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2720476 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2720476[ORGN]\n",
            "— No hits for taxid 2720476\n",
            "⏳ BLASTP vs nr | taxid=2804519 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2804519[ORGN]\n",
            "✅ taxid 2804519: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2812629 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2812629[ORGN]\n",
            "— No hits for taxid 2812629\n",
            "⏳ BLASTP vs nr | taxid=2812896 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2812896[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request ADJZ6CR3015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— No hits for taxid 2812896\n",
            "⏳ BLASTP vs nr | taxid=2814275 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2814275[ORGN]\n",
            "✅ taxid 2814275: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2823368 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2823368[ORGN]\n",
            "— No hits for taxid 2823368\n",
            "⏳ BLASTP vs nr | taxid=2833567 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2833567[ORGN]\n",
            "— No hits for taxid 2833567\n",
            "⏳ BLASTP vs nr | taxid=2847862 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2847862[ORGN]\n",
            "✅ taxid 2847862: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2857077 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2857077[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request ADMSRCN7015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ taxid 2857077: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2886104 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2886104[ORGN]\n",
            "✅ taxid 2886104: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2886352 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2886352[ORGN]\n",
            "✅ taxid 2886352: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2908145 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2908145[ORGN]\n",
            "✅ taxid 2908145: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2908146 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2908146[ORGN]\n",
            "✅ taxid 2908146: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=2908147 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2908147[ORGN]\n",
            "✅ taxid 2908147: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2922243 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2922243[ORGN]\n",
            "— No hits for taxid 2922243\n",
            "⏳ BLASTP vs nr | taxid=2929567 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2929567[ORGN]\n",
            "✅ taxid 2929567: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2964529 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2964529[ORGN]\n",
            "✅ taxid 2964529: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=2971801 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid2971801[ORGN]\n",
            "✅ taxid 2971801: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=3018267 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3018267[ORGN]\n",
            "— No hits for taxid 3018267\n",
            "⏳ BLASTP vs nr | taxid=3023278 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3023278[ORGN]\n",
            "✅ taxid 3023278: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3050158 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3050158[ORGN]\n",
            "✅ taxid 3050158: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3064516 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3064516[ORGN]\n",
            "✅ taxid 3064516: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=3069706 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3069706[ORGN]\n",
            "✅ taxid 3069706: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3073602 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3073602[ORGN]\n",
            "— No hits for taxid 3073602\n",
            "⏳ BLASTP vs nr | taxid=3075395 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3075395[ORGN]\n",
            "✅ taxid 3075395: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3081323 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3081323[ORGN]\n",
            "✅ taxid 3081323: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3083423 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3083423[ORGN]\n",
            "✅ taxid 3083423: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3088171 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3088171[ORGN]\n",
            "✅ taxid 3088171: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3107143 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3107143[ORGN]\n",
            "— No hits for taxid 3107143\n",
            "⏳ BLASTP vs nr | taxid=3350582 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3350582[ORGN]\n",
            "✅ taxid 3350582: kept 2 sequences\n",
            "⏳ BLASTP vs nr | taxid=3396280 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid3396280[ORGN]\n",
            "— No hits for taxid 3396280\n",
            "💾 Multi-FASTA: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L0-species_diversity/per_taxid_top_hits.fasta\n",
            "📑 Table: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L0-species_diversity/per_taxid_hits.tsv\n",
            "🗂 XML files saved: 122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts fasta sequences from XML files"
      ],
      "metadata": {
        "id": "cLZJwzdZF3aF"
      },
      "id": "cLZJwzdZF3aF"
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Blast import NCBIXML\n",
        "from Bio import Entrez\n",
        "from pathlib import Path\n",
        "import re, time\n",
        "\n",
        "# REQUIRED\n",
        "Entrez.email = \"your_email@university.edu\"   # <-- change me\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"       # optional but helpful\n",
        "\n",
        "# Paths (match your screenshots)\n",
        "# DATA_DIR  = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\")\n",
        "# OUT_FASTA = DATA_DIR / \"blast_hits.fasta\"\n",
        "\n",
        "# Use the OUTPUT_DIR defined in the previous cell\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "else:\n",
        "    # Fallback if the variable is not set\n",
        "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L0-species_diversity\")\n",
        "\n",
        "OUT_FASTA = OUTPUT_DIR / \"blast_hits.fasta\"\n",
        "\n",
        "\n",
        "# Find XML files (your files look like 'blastp_nr_taxid*.xml')\n",
        "# XMLS = sorted(DATA_DIR.glob(\"blastp_nr_taxid*.xml\"))\n",
        "# if not XMLS:\n",
        "#     XMLS = sorted(DATA_DIR.glob(\"*.xml\"))\n",
        "# assert XMLS, f\"No XML files found in {DATA_DIR}\"\n",
        "\n",
        "# Look for XML files in the OUTPUT_DIR\n",
        "XMLS = sorted(OUTPUT_DIR.glob(\"blastp_nr_taxid*.xml\"))\n",
        "if not XMLS:\n",
        "    XMLS = sorted(OUTPUT_DIR.glob(\"*.xml\")) # Fallback to any xml if specific not found\n",
        "\n",
        "assert XMLS, f\"No XML files found in {OUTPUT_DIR}\"\n",
        "\n",
        "\n",
        "MAX_HITS_PER_XML = 5     # top N alignments per XML\n",
        "BATCH = 50               # efetch batch size\n",
        "SLEEP = 0.25             # courtesy to NCBI\n",
        "\n",
        "def parse_top_accessions(xml_path, top_n=5):\n",
        "    \"\"\"Return list of accession strings from top alignments in one BLAST XML.\"\"\"\n",
        "    with open(xml_path) as h:\n",
        "        rec = NCBIXML.read(h)\n",
        "    accs = []\n",
        "    for aln in rec.alignments[:top_n]:\n",
        "        # Prefer parser's accession if available; else regex from id/def/title\n",
        "        acc = getattr(aln, \"accession\", None)\n",
        "        if not acc:\n",
        "            for field in (aln.hit_id, aln.hit_def, aln.title):\n",
        "                m = re.search(r\"([A-Z]{1,5}_?\\d+(?:\\.\\d+)?)\", field or \"\")\n",
        "                if m:\n",
        "                    acc = m.group(1); break\n",
        "        if acc:\n",
        "            accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "def normalize_pair(acc):\n",
        "    \"\"\"Return (base, versioned) forms so XOB97566 and XOB97566.1 both match.\"\"\"\n",
        "    base = acc.split(\".\")[0]\n",
        "    ver  = acc if \".\" in acc else None\n",
        "    return base, ver\n",
        "\n",
        "def fetch_fasta_batched(accessions):\n",
        "    \"\"\"Fetch FASTA for a list of accessions; store under both base & version keys.\"\"\"\n",
        "    out = {}\n",
        "    accs = [a for a in set(accessions) if a]\n",
        "    for i in range(0, len(accs), BATCH):\n",
        "        chunk = accs[i:i+BATCH]\n",
        "        try:\n",
        "            handle = Entrez.efetch(db=\"protein\", id=\",\".join(chunk), rettype=\"fasta\", retmode=\"text\")\n",
        "            text = handle.read(); handle.close()\n",
        "            for block in [b for b in text.strip().split(\">\") if b]:\n",
        "                header, *seq_lines = block.splitlines()\n",
        "                header = header.strip()\n",
        "                seq = \"\\n\".join(seq_lines).strip()\n",
        "                token = header.split()[0]        # e.g., XOB97566.1\n",
        "                base = token.split(\".\")[0]\n",
        "                fasta = f\">{header}\\n{seq}\\n\"\n",
        "                out[token] = fasta\n",
        "                out[base]  = fasta\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] efetch failed for {chunk}: {e}\")\n",
        "        time.sleep(SLEEP)\n",
        "    return out\n",
        "\n",
        "# --- Collect top accessions from all XMLs\n",
        "wanted = []\n",
        "for x in XMLS:\n",
        "    accs = parse_top_accessions(x, MAX_HITS_PER_XML)\n",
        "    if accs:\n",
        "        wanted.extend(accs)\n",
        "    else:\n",
        "        print(f\"— {x.name}: no alignments or no accessions parsed\")\n",
        "\n",
        "# Also include versionless forms to improve retrieval\n",
        "expanded = []\n",
        "for a in set(wanted):\n",
        "    base, ver = normalize_pair(a)\n",
        "    expanded.append(base)\n",
        "    if ver: expanded.append(ver)\n",
        "\n",
        "# --- Fetch FASTA\n",
        "acc_to_fasta = fetch_fasta_batched(expanded)\n",
        "\n",
        "# --- Write combined FASTA (keep original headers; add source xml in a comment line)\n",
        "written = 0\n",
        "with open(OUT_FASTA, \"w\") as out:\n",
        "    for x in XMLS:\n",
        "        accs = parse_top_accessions(x, MAX_HITS_PER_XML)\n",
        "        for acc in accs:\n",
        "            # try versioned, then base\n",
        "            fa = acc_to_fasta.get(acc) or acc_to_fasta.get(acc.split(\".\")[0])\n",
        "            if not fa:\n",
        "                print(f\"[miss] No FASTA for {acc} (from {x.name})\")\n",
        "                continue\n",
        "            # insert a comment line indicating source XML\n",
        "            header, *seq_lines = fa.strip().splitlines()\n",
        "            out.write(header + f\"  ; source={x.name}\\n\")\n",
        "            out.write(\"\\n\".join(seq_lines) + \"\\n\")\n",
        "            written += 1\n",
        "\n",
        "print(f\"💾 Wrote {written} sequences to {OUT_FASTA}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BeVaPs_kWxZ",
        "outputId": "33dd50cc-cb05-43b6-cc85-d6ed17749ad1"
      },
      "id": "8BeVaPs_kWxZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Wrote 12 sequences to /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L0-species_diversity/blast_hits.fasta\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}