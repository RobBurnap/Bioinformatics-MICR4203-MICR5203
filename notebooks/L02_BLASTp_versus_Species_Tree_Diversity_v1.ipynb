{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobBurnap/Bioinformatics-MICR4203-MICR5203/blob/main/notebooks/L02_BLASTp_versus_Species_Tree_Diversity_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275544ee",
      "metadata": {
        "id": "275544ee"
      },
      "source": [
        "\n",
        "# BIOINFO4/5203 —\n",
        "species diversity BLASTp:  Need two files in your"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A. Mount Google Drive, Import Coding Libraries Necessary for Running Subsequent Code"
      ],
      "metadata": {
        "id": "fU6YPxFnnl1Y"
      },
      "id": "fU6YPxFnnl1Y"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "269c6b87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "269c6b87",
        "outputId": "bfea8a15-f0ee-4af7-fd30-f78476811187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/3.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "✅ Dependencies installed & Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install FIRST, then import\n",
        "%pip install -q biopython       # Install the Biopython package quietly (-q suppresses most output) so we can work with biological sequence files\n",
        "\n",
        "from google.colab import drive  # Import the module that lets Colab interact with Google Drive\n",
        "drive.mount('/content/drive')   # Mount your Google Drive so it appears in Colab's file system under /content/drive\n",
        "\n",
        "import os, pandas as pd          # Import 'os' for file/directory operations, and pandas for working with data tables\n",
        "from Bio import SeqIO            # Import SeqIO from Biopython for reading/writing biological sequence files (FASTA, GenBank, etc.)\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib's plotting library to create figures and graphs\n",
        "\n",
        "print(\"✅ Dependencies installed & Drive mounted.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fa44d1",
      "metadata": {
        "id": "42fa44d1"
      },
      "source": [
        "\n",
        "## B. Course folders: Define the course folders for places to load data to be processed and output to be saved\n",
        "\n",
        "Edit only `LECTURE_CODE` and `TOPIC` if needed. All inputs will live in `Data/LECTURE_TOPIC` and outputs in `Outputs/LECTURE_TOPIC`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66b0e9d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66b0e9d7",
        "outputId": "01e2ed18-93b9-4526-ce35-5a49a48af9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 COURSE_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\n",
            "📁 DATA_DIR   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L02-BLASTp_diversity\n",
            "📁 OUTPUT_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L02-BLASTp_diversity\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Course folder config (customize LECTURE_CODE/TOPIC only) ---\n",
        "COURSE_DIR   = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "LECTURE_CODE = \"L02-BLASTp\"            # change per week (e.g., L02, L03, ...)\n",
        "TOPIC        = \"diversity\"    # short slug for the exercise\n",
        "\n",
        "# Derived paths (do not change)\n",
        "DATA_DIR   = f\"{COURSE_DIR}/Data/{LECTURE_CODE}_{TOPIC}\"\n",
        "OUTPUT_DIR = f\"{COURSE_DIR}/Outputs/{LECTURE_CODE}_{TOPIC}\"\n",
        "\n",
        "# Create folder structure if missing\n",
        "for p in [f\"{COURSE_DIR}/Data\", f\"{COURSE_DIR}/Outputs\", f\"{COURSE_DIR}/Notebooks\", DATA_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"📁 COURSE_DIR :\", COURSE_DIR)\n",
        "print(\"📁 DATA_DIR   :\", DATA_DIR)\n",
        "print(\"📁 OUTPUT_DIR :\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C.\n",
        "\n"
      ],
      "metadata": {
        "id": "ITJ9NS_HlbIU"
      },
      "id": "ITJ9NS_HlbIU"
    },
    {
      "cell_type": "markdown",
      "source": [
        " multi-FASTA of top hits per TaxID (one or more sequences per taxon). The cell below:\n",
        "\t•\tuses your existing folders (Data/L0-species_diversity for input; writes to the same folder unless OUTPUT_DIR is already set),\n",
        "\t•\treads query_proteins.fasta and taxids.txt,\n",
        "\t•\tfor each TaxID, runs a separate BLAST restricted to that taxon (txid####[ORGN]), so we can attribute hits unambiguously,\n",
        "\t•\tgrabs the top N accessions from each BLAST,\n",
        "\t•\tfetches their protein FASTA sequences,\n",
        "\t•\twrites:\n",
        "\t•\tper_taxid_top_hits.fasta (all sequences, grouped by taxid in headers),\n",
        "\t•\tper_taxid_hits.tsv (who came from which taxid, evalue, %id, etc.),\n",
        "\t•\toptional one FASTA per taxid (toggle with WRITE_SPLIT_FASTA)."
      ],
      "metadata": {
        "id": "ADIsRNvmtr3K"
      },
      "id": "ADIsRNvmtr3K"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLAST (per TaxID) -> collect top protein hits -> write multi-FASTA + TSV (adaptive search, matrix+gapcosts safe) ---\n",
        "from Bio import Entrez, SeqIO\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from pathlib import Path\n",
        "import io, csv, re, time, sys\n",
        "\n",
        "# ==== required ====\n",
        "Entrez.email = \"you@university.edu\"   # <-- set your email\n",
        "\n",
        "# ==== Alogorithm 'knobs' you can tweak ====\n",
        "TOP_HITS_PER_TAXID = 1\n",
        "EVALUE_STEPS       = [1e-5, 1e-2, 0.1]          # strict -> moderate -> permissive\n",
        "HITLIST_SIZE       = max(200, TOP_HITS_PER_TAXID*10)\n",
        "WRITE_SPLIT_FASTA  = False # if True, also writes one FASTA file per taxid (in addition to combined multi sequence file)\n",
        "SLEEP_BETWEEN_CALLS = 0.3\n",
        "FORCE_MODE = 'auto'                              # 'auto' | 'blastp' | 'blastx'\n",
        "\n",
        "# Matrix + gap-costs mapping (what NCBI expects)\n",
        "MATRIX_DEFAULT = \"BLOSUM62\"\n",
        "GAPCOSTS_BY_MATRIX = {\n",
        "    \"BLOSUM62\": \"11 1\",\n",
        "    \"BLOSUM45\": \"15 2\",\n",
        "    \"PAM30\":    \"9 1\",\n",
        "    \"PAM70\":    \"10 1\",\n",
        "}\n",
        "\n",
        "# ==== paths ====\n",
        "if 'DATA_DIR' in globals(): DATA_DIR = Path(DATA_DIR)\n",
        "else: DATA_DIR = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\")\n",
        "\n",
        "if 'OUTPUT_DIR' in globals(): OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "else: OUTPUT_DIR = DATA_DIR\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROT_FASTA = DATA_DIR / \"query_protein.fasta\"\n",
        "NUC_FASTA  = DATA_DIR / \"query.fasta\"\n",
        "TAXIDS_TXT = DATA_DIR / \"taxids.txt\"\n",
        "\n",
        "# ---- basic file checks ----\n",
        "if not (PROT_FASTA.exists() or NUC_FASTA.exists()):\n",
        "    raise FileNotFoundError(f\"Neither {PROT_FASTA} nor {NUC_FASTA} found in {DATA_DIR}.\")\n",
        "if not TAXIDS_TXT.exists():\n",
        "    raise FileNotFoundError(f\"{TAXIDS_TXT} not found in {DATA_DIR}.\")\n",
        "\n",
        "# ==== helpers: accession parsing & normalization ====\n",
        "def extract_accession(hit_id, hit_def, accession_attr):\n",
        "    if accession_attr: return accession_attr.strip()\n",
        "    for field in (hit_id, hit_def):\n",
        "        m = re.search(r\"([A-Z]{1,5}_?\\d+(?:\\.\\d+)?)\", field or \"\")\n",
        "        if m: return m.group(1)\n",
        "    return (hit_id or \"unknown\").strip()\n",
        "\n",
        "def _norm_keys(acc_token: str):\n",
        "    acc_token = acc_token.strip()\n",
        "    base = acc_token.split(\".\", 1)[0]\n",
        "    return {acc_token, base} if \".\" in acc_token else {acc_token, f\"{base}.1\"}\n",
        "\n",
        "def fetch_protein_fasta(accessions):\n",
        "    out = {}\n",
        "    batch = list({a for a in accessions if a and a != \"unknown\"})\n",
        "    while batch:\n",
        "        chunk = batch[:50]; batch = batch[50:]\n",
        "        try:\n",
        "            h = Entrez.efetch(db=\"protein\", id=\",\".join(chunk), rettype=\"fasta\", retmode=\"text\")\n",
        "            txt = h.read(); h.close()\n",
        "            parts = [t for t in txt.strip().split(\">\") if t]\n",
        "            for rec_txt in parts:\n",
        "                header, *seq_lines = rec_txt.splitlines()\n",
        "                token = header.split()[0]\n",
        "                fasta_txt = \">\" + header + \"\\n\" + \"\\n\".join(seq_lines) + \"\\n\"\n",
        "                for k in _norm_keys(token):\n",
        "                    out[k] = fasta_txt\n",
        "        except Exception as e:\n",
        "            sys.stderr.write(f\"[warn] efetch failed for {chunk}: {e}\\n\")\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "    return out\n",
        "\n",
        "# ==== robust query loader/detector ====\n",
        "DNA_ALPHABET = set(\"ACGTNUWSMKRYBDHV-\")\n",
        "def detect_is_protein(seq_upper: str) -> bool:\n",
        "    letters = [c for c in seq_upper if c.isalpha() or c == '*']\n",
        "    return any((c not in DNA_ALPHABET) for c in letters)\n",
        "\n",
        "def load_query():\n",
        "    if PROT_FASTA.exists():\n",
        "        rec = next(SeqIO.parse(str(PROT_FASTA), \"fasta\"))\n",
        "        seq = str(rec.seq).upper()\n",
        "        decided = 'blastp' if detect_is_protein(seq) else 'blastx'\n",
        "        if FORCE_MODE in ('blastp','blastx'): decided = FORCE_MODE\n",
        "        return decided, seq.replace(\"*\",\"\"), f\"{PROT_FASTA.name}:{rec.id}\"\n",
        "    rec = next(SeqIO.parse(str(NUC_FASTA), \"fasta\"))\n",
        "    seq = str(rec.seq).upper()\n",
        "    decided = 'blastp' if detect_is_protein(seq) else 'blastx'\n",
        "    if FORCE_MODE in ('blastp','blastx'): decided = FORCE_MODE\n",
        "    if decided == 'blastp' and NUC_FASTA.name == \"query.fasta\":\n",
        "        print(\"⚠️  Detected protein sequence in 'query.fasta'; using BLASTP.\")\n",
        "    return decided, seq.replace(\"*\",\"\"), f\"{NUC_FASTA.name}:{rec.id}\"\n",
        "\n",
        "mode, query_seq, query_label = load_query()\n",
        "print(f\"📄 Query source: {query_label}\")\n",
        "print(f\"🧪 Mode chosen: {mode.upper()} vs nr  | length={len(query_seq)}\")\n",
        "\n",
        "# ==== load TaxIDs ====\n",
        "taxids = [t.strip() for t in TAXIDS_TXT.read_text().splitlines() if t.strip().isdigit()]\n",
        "if not taxids: raise ValueError(\"taxids.txt is empty or contains no numeric TaxIDs.\")\n",
        "print(f\"🧬 Loaded {len(taxids)} TaxIDs\")\n",
        "\n",
        "# ==== ADAPTIVE BLAST (matrix + gapcosts always paired) ====\n",
        "def run_single_blast_adaptive(seq, taxid, program=\"blastp\",\n",
        "                              hitlist=HITLIST_SIZE, matrix=MATRIX_DEFAULT):\n",
        "    q = f\"txid{taxid}[ORGN]\"\n",
        "    last_xml, last_rec = None, None\n",
        "    cur_matrix = matrix\n",
        "    for e in EVALUE_STEPS:\n",
        "        gapcosts = GAPCOSTS_BY_MATRIX.get(cur_matrix, GAPCOSTS_BY_MATRIX[\"BLOSUM62\"])\n",
        "        print(f\"⏳ {program.upper()} vs nr | taxid={taxid} | E={e} | hits={hitlist} | matrix={cur_matrix} | gaps={gapcosts}\\n   ENTREZ_QUERY: {q}\")\n",
        "        h = NCBIWWW.qblast(program=program, database=\"nr\", sequence=seq,\n",
        "                           expect=e, entrez_query=q,\n",
        "                           hitlist_size=hitlist, descriptions=hitlist,\n",
        "                           alignments=hitlist, matrix_name=cur_matrix,\n",
        "                           gapcosts=gapcosts)\n",
        "        xml = h.read(); h.close()\n",
        "        try:\n",
        "            rec = NCBIXML.read(io.StringIO(xml))\n",
        "        except Exception:\n",
        "            parser = NCBIXML.parse(io.StringIO(xml))\n",
        "            rec = next(parser, None)\n",
        "        last_xml, last_rec = xml, rec\n",
        "        if rec and rec.alignments:\n",
        "            return rec, xml, e, cur_matrix\n",
        "        # One relaxation step: switch to BLOSUM45 (with its proper gapcosts) after the moderate E\n",
        "        if e == 1e-2 and cur_matrix == \"BLOSUM62\":\n",
        "            cur_matrix = \"BLOSUM45\"\n",
        "    return last_rec, last_xml, e, cur_matrix\n",
        "\n",
        "# ==== main loop -> XML + top hits -> fetch FASTA ====\n",
        "all_rows = []\n",
        "per_taxid_fastas = {}\n",
        "xml_paths = []\n",
        "\n",
        "for i, tid in enumerate(taxids, 1):\n",
        "    try:\n",
        "        record, xml, e_used, m_used = run_single_blast_adaptive(\n",
        "            query_seq, tid, program=(\"blastp\" if mode==\"blastp\" else \"blastx\"))\n",
        "        print(f\"   ↳ used E={e_used}, matrix={m_used}\")\n",
        "        xml_file = OUTPUT_DIR / f\"{mode}_nr_taxid{tid}.xml\"\n",
        "        xml_file.write_text(xml); xml_paths.append(xml_file)\n",
        "\n",
        "        if not record or not record.alignments:\n",
        "            print(f\"— No hits for taxid {tid}\")\n",
        "            continue\n",
        "\n",
        "        hsps = []\n",
        "        for aln in record.alignments:\n",
        "            best = sorted(aln.hsps, key=lambda h: (h.expect, -h.identities))[0]\n",
        "            acc = extract_accession(aln.hit_id, aln.hit_def, getattr(aln, \"accession\", None))\n",
        "            pct_id = 100.0 * best.identities / best.align_length if best.align_length else 0.0\n",
        "            hsps.append((aln, best, acc, pct_id))\n",
        "        hsps.sort(key=lambda t: (t[1].expect, -t[3]))\n",
        "        keep = hsps[:TOP_HITS_PER_TAXID]\n",
        "\n",
        "        accs = [acc for _,_,acc,_ in keep]\n",
        "        acc_to_fa = fetch_protein_fasta(accs)\n",
        "\n",
        "        per_taxid_fastas.setdefault(tid, [])\n",
        "        kept_now = 0\n",
        "        for aln, best, acc, pct in keep:\n",
        "            fa = None\n",
        "            for k in _norm_keys(acc):\n",
        "                fa = acc_to_fa.get(k)\n",
        "                if fa: break\n",
        "            if not fa:\n",
        "                sys.stderr.write(f\"[miss] No FASTA for {acc} (taxid {tid})\\n\")\n",
        "                continue\n",
        "            lines = fa.strip().splitlines()\n",
        "            header = lines[0][1:]\n",
        "            seq = \"\\n\".join(lines[1:])\n",
        "            new_header = f\">taxid:{tid}|acc:{acc}|e:{best.expect:.2e}|pid:{pct:.2f}|len:{best.align_length} {header}\"\n",
        "            per_taxid_fastas[tid].append(new_header + \"\\n\" + seq + \"\\n\"); kept_now += 1\n",
        "\n",
        "            all_rows.append([\n",
        "                tid, acc, aln.title, aln.length, best.expect,\n",
        "                best.identities, best.align_length, round(pct,2),\n",
        "                min(best.query_start,best.query_end), max(best.query_start,best.query_end),\n",
        "                min(best.sbjct_start,best.sbjct_end), max(best.sbjct_start,best.sbjct_end)\n",
        "            ])\n",
        "        print(f\"✅ taxid {tid}: kept {kept_now} sequences\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ taxid {tid} failed: {e}\")\n",
        "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "\n",
        "# ==== write combined multi-FASTA + per-taxid FASTAs ====\n",
        "multi_fa = OUTPUT_DIR / \"per_taxid_top_hits.fasta\"\n",
        "with open(multi_fa, \"w\") as fh:\n",
        "    for tid in taxids:\n",
        "        for fa in per_taxid_fastas.get(tid, []):\n",
        "            fh.write(fa)\n",
        "print(f\"💾 Multi-FASTA: {multi_fa}\")\n",
        "\n",
        "if WRITE_SPLIT_FASTA:\n",
        "    for tid, fas in per_taxid_fastas.items():\n",
        "        p = OUTPUT_DIR / f\"taxid_{tid}_top_hits.fasta\"\n",
        "        with open(p, \"w\") as fh:\n",
        "            for fa in fas: fh.write(fa)\n",
        "    print(\"💾 Also wrote per-taxid FASTAs\")\n",
        "\n",
        "# ==== write table ====\n",
        "tsv = OUTPUT_DIR / \"per_taxid_hits.tsv\"\n",
        "with open(tsv, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f, delimiter=\"\\t\")\n",
        "    w.writerow([\"taxid\",\"accession\",\"title\",\"subject_length\",\"evalue\",\"identities\",\"align_len\",\"pct_identity\",\"q_start\",\"q_end\",\"s_start\",\"s_end\"])\n",
        "    w.writerows(all_rows)\n",
        "print(f\"📑 Table: {tsv}\")\n",
        "print(f\"🗂 XML files saved: {len(xml_paths)}\")"
      ],
      "metadata": {
        "id": "ctgn3Y6EpkXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef29d72-a200-4e07-ca4f-414f5fde9c32"
      },
      "id": "ctgn3Y6EpkXa",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Query source: query_protein.fasta:Nqo13-mod\n",
            "🧪 Mode chosen: BLASTP vs nr  | length=481\n",
            "🧬 Loaded 4 TaxIDs\n",
            "⏳ BLASTP vs nr | taxid=45264 | E=1e-05 | hits=200 | matrix=BLOSUM62 | gaps=11 1\n",
            "   ENTREZ_QUERY: txid45264[ORGN]\n",
            "   ↳ used E=1e-05, matrix=BLOSUM62\n",
            "✅ taxid 45264: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=1423 | E=1e-05 | hits=200 | matrix=BLOSUM62 | gaps=11 1\n",
            "   ENTREZ_QUERY: txid1423[ORGN]\n",
            "   ↳ used E=1e-05, matrix=BLOSUM62\n",
            "✅ taxid 1423: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=9913 | E=1e-05 | hits=200 | matrix=BLOSUM62 | gaps=11 1\n",
            "   ENTREZ_QUERY: txid9913[ORGN]\n",
            "   ↳ used E=1e-05, matrix=BLOSUM62\n",
            "✅ taxid 9913: kept 1 sequences\n",
            "⏳ BLASTP vs nr | taxid=1416614 | E=1e-05 | hits=200 | matrix=BLOSUM62 | gaps=11 1\n",
            "   ENTREZ_QUERY: txid1416614[ORGN]\n",
            "   ↳ used E=1e-05, matrix=BLOSUM62\n",
            "✅ taxid 1416614: kept 1 sequences\n",
            "💾 Multi-FASTA: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L02-BLASTp_diversity/per_taxid_top_hits.fasta\n",
            "📑 Table: /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L02-BLASTp_diversity/per_taxid_hits.tsv\n",
            "🗂 XML files saved: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Taxon Map with common names & lineage (incl. Proteobacteria class**"
      ],
      "metadata": {
        "id": "cLZJwzdZF3aF"
      },
      "id": "cLZJwzdZF3aF"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Clean + augment a taxon map for students ===\n",
        "from Bio import Entrez\n",
        "from pathlib import Path\n",
        "import pandas as pd, re, time, sys, os, glob\n",
        "\n",
        "# ---- Paths (edit BASE if needed)\n",
        "BASE = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "DATA_DIR   = os.path.join(BASE, \"Data\",   \"L02-BLASTp_diversity\")\n",
        "OUTPUT_DIR = os.path.join(BASE, \"Outputs\",\"L02-BLASTp_diversity\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Try to find the input CSV\n",
        "cands = [\n",
        "    os.path.join(DATA_DIR, \"Bioinformatics-taxon_map.csv\"),\n",
        "    os.path.join(DATA_DIR, \"bioinformatics-taxon_map.csv\"),\n",
        "    os.path.join(DATA_DIR, \"taxon_map.csv\"),\n",
        "]\n",
        "cands += glob.glob(os.path.join(DATA_DIR, \"*taxon*map*.csv\"))\n",
        "INPUT_CSV = next((p for p in cands if os.path.exists(p)), None)\n",
        "assert INPUT_CSV, f\"Could not find a taxon map CSV in {DATA_DIR}\"\n",
        "\n",
        "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"Bioinformatics-taxon_map_annotated.csv\")\n",
        "print(\"📄 Input :\", INPUT_CSV)\n",
        "print(\"📂 Out   :\", OUTPUT_CSV)\n",
        "\n",
        "Entrez.email = \"you@university.edu\"   # <<-- set your email\n",
        "SLEEP = 0.25\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def fetch_node(tid: str):\n",
        "    try:\n",
        "        h = Entrez.efetch(db=\"taxonomy\", id=str(tid), retmode=\"xml\")\n",
        "        recs = Entrez.read(h); h.close()\n",
        "        return recs[0] if recs else {}\n",
        "    except Exception as e:\n",
        "        sys.stderr.write(f\"[warn] efetch({tid}): {e}\\n\")\n",
        "        return {}\n",
        "\n",
        "def lineage_lookup(node, rank):\n",
        "    for x in node.get(\"LineageEx\", []):\n",
        "        if x.get(\"Rank\") == rank:\n",
        "            return x.get(\"ScientificName\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def get_common_name(node):\n",
        "    cn = node.get(\"CommonName\") or \"\"\n",
        "    if not cn:\n",
        "        other = node.get(\"OtherNames\", {}) or {}\n",
        "        cn = other.get(\"CommonName\") or other.get(\"GenbankCommonName\") or \"\"\n",
        "        if not cn and isinstance(other.get(\"Synonym\"), list) and other[\"Synonym\"]:\n",
        "            cn = other[\"Synonym\"][0]\n",
        "    return cn\n",
        "\n",
        "def proteo_group_from_class(cls):\n",
        "    cl = (cls or \"\").lower()\n",
        "    if \"alphaproteobacteria\" in cl: return \"Alpha\"\n",
        "    if \"betaproteobacteria\"  in cl: return \"Beta\"\n",
        "    if \"gammaproteobacteria\" in cl: return \"Gamma\"\n",
        "    if \"deltaproteobacteria\" in cl: return \"Delta\"\n",
        "    if \"epsilonproteobacteria\" in cl or \"campylobacteria\" in cl: return \"Epsilon\"\n",
        "    return \"\"\n",
        "\n",
        "def infer_category(sk, ph, name, cls, common):\n",
        "    # normalize to string\n",
        "    sk = str(sk[0] if isinstance(sk, list) else sk or \"\").lower()\n",
        "    ph = str(ph[0] if isinstance(ph, list) else ph or \"\").lower()\n",
        "    nm = str(name[0] if isinstance(name, list) else name or \"\").lower()\n",
        "    cl = str(cls[0] if isinstance(cls, list) else cls or \"\").lower()\n",
        "    cm = str(common[0] if isinstance(common, list) else common or \"\").lower()\n",
        "\n",
        "    if sk == \"bacteria\":\n",
        "        if \"proteobacteria\" in ph:\n",
        "            g = proteo_group_from_class(cl)\n",
        "            return f\"Bacteria (Proteobacteria {g})\" if g else \"Bacteria (Proteobacteria)\"\n",
        "        return \"Bacteria\"\n",
        "    if sk == \"archaea\":\n",
        "        return \"Archaea\"\n",
        "\n",
        "    # Eukaryotes\n",
        "    if ph in {\"streptophyta\",\"tracheophyta\",\"magnoliophyta\",\"bryophyta\"} or any(x in nm for x in [\"arabidopsis\",\"oryza\",\"zea mays\"]):\n",
        "        return \"Plants\"\n",
        "    if ph == \"chlorophyta\" or \"chlamydomonas\" in nm:\n",
        "        return \"Algae (green)\"\n",
        "    if \"bacillariophyta\" in ph or \"phaeophyceae\" in cl:\n",
        "        return \"Algae (other)\"\n",
        "    if ph in {\"ascomycota\",\"basidiomycota\",\"chytridiomycota\",\"mucoromycota\"} or any(x in nm for x in [\"saccharomyces\",\"neurospora\",\"pombe\"]):\n",
        "        return \"Fungi\"\n",
        "    if any(x in nm for x in [\"homo sapiens\",\"pan \",\"gorilla\",\"pongo\",\"papio\",\"macaca\"]):\n",
        "        return \"Primates\"\n",
        "    if any(x in nm for x in [\"mus musculus\",\"rattus norvegicus\",\"bos taurus\",\"canis lupus\",\"felis catus\",\"sus scrofa\"]):\n",
        "        return \"Mammals\"\n",
        "    if ph in {\"apicomplexa\",\"euglenozoa\",\"ciliophora\",\"amoebozoa\",\"parabasalia\",\"haptophyta\"}:\n",
        "        return \"Protists\"\n",
        "    return \"Other Eukaryotes\"\n",
        "\n",
        "# ---------- load & normalize taxid ----------\n",
        "df_raw = pd.read_csv(INPUT_CSV)\n",
        "\n",
        "# Find the taxid column (case-insensitive)\n",
        "taxid_col = next((c for c in df_raw.columns if c.lower().strip() == \"taxid\"), None)\n",
        "if taxid_col is None:\n",
        "    raise ValueError(\"Input CSV must contain a 'taxid' column (any capitalization).\")\n",
        "\n",
        "df = df_raw.copy()\n",
        "\n",
        "# Clean taxid to plain digits (handles floats '543639.0', scientific, spaces)\n",
        "def scrub_tid(val):\n",
        "    s = \"\" if pd.isna(val) else str(val)\n",
        "    m = re.search(r\"\\d+\", s)\n",
        "    return m.group(0) if m else \"\"\n",
        "\n",
        "df[\"taxid\"] = df[taxid_col].apply(scrub_tid)\n",
        "\n",
        "# Carry through any existing names if present\n",
        "if \"canonical_name\" not in df.columns: df[\"canonical_name\"] = df.get(\"organism_name\", \"\")\n",
        "if \"input_name\"     not in df.columns: df[\"input_name\"]     = df.get(\"organism_name\", \"\")\n",
        "\n",
        "# Prepare new columns\n",
        "for col in (\"common_name\",\"superkingdom\",\"phylum\",\"class\",\"proteo_group\",\"category\",\"status\"):\n",
        "    if col not in df.columns: df[col] = \"\"\n",
        "\n",
        "# ---------- fetch & fill ----------\n",
        "for i, row in df.iterrows():\n",
        "    tid = row[\"taxid\"]\n",
        "    if not tid:\n",
        "        df.at[i,\"status\"] = \"no_taxid\"\n",
        "        continue\n",
        "    node = fetch_node(tid)\n",
        "    if not node:\n",
        "        df.at[i,\"status\"] = \"lookup_failed\"; time.sleep(SLEEP); continue\n",
        "\n",
        "    sci = node.get(\"ScientificName\",\"\")\n",
        "    df.at[i,\"canonical_name\"] = row.get(\"canonical_name\") or sci\n",
        "    df.at[i,\"common_name\"]    = get_common_name(node)\n",
        "    sk = lineage_lookup(node,\"superkingdom\"); ph = lineage_lookup(node,\"phylum\"); cl = lineage_lookup(node,\"class\")\n",
        "    df.at[i,\"superkingdom\"]   = sk\n",
        "    df.at[i,\"phylum\"]         = ph\n",
        "    df.at[i,\"class\"]          = cl\n",
        "    df.at[i,\"proteo_group\"]   = proteo_group_from_class(cl)\n",
        "    df.at[i,\"category\"]       = infer_category(sk, ph, sci, cl, df.at[i,\"common_name\"])\n",
        "    df.at[i,\"status\"]         = \"ok\"\n",
        "    time.sleep(SLEEP)\n",
        "\n",
        "# ---------- tidy columns for students ----------\n",
        "keep_order = [\n",
        "    \"taxid\",\"canonical_name\",\"common_name\",\"category\",\n",
        "    \"superkingdom\",\"phylum\",\"class\",\"proteo_group\",\"status\"\n",
        "]\n",
        "# include input_name if present and different\n",
        "if \"input_name\" in df.columns: keep_order.insert(1,\"input_name\")\n",
        "\n",
        "df_clean = df[keep_order].copy()\n",
        "\n",
        "# Sort for readability\n",
        "def sortkey(row):\n",
        "    dom = row[\"superkingdom\"] or \"\"\n",
        "    cat = row[\"category\"] or \"\"\n",
        "    name = row[\"canonical_name\"] or \"\"\n",
        "    return (dom, cat, name)\n",
        "df_clean = df_clean.sort_values(by=keep_order, key=lambda col: col.map(lambda x: str(x))).reset_index(drop=True)\n",
        "\n",
        "df_clean.to_csv(OUTPUT_CSV, index=False)\n",
        "print(\"✅ Wrote:\", OUTPUT_CSV)\n",
        "print(df_clean.head(12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Uv4N14bydn",
        "outputId": "761b3327-7f68-4031-db97-eb516cb469f1"
      },
      "id": "G9Uv4N14bydn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Input : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L02-BLASTp_diversity/Bioinformatics-taxon_map.csv\n",
            "📂 Out   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L02-BLASTp_diversity/Bioinformatics-taxon_map_annotated.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}