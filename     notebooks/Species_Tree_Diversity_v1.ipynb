{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobBurnap/Bioinformatics-MICR4203-MICR5203/blob/main/%20%20%20%20notebooks/Species_Tree_Diversity_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275544ee",
      "metadata": {
        "id": "275544ee"
      },
      "source": [
        "\n",
        "# BIOINFO4/5203 ‚Äî\n",
        "species diveristy:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A. Mount Google Drive, Import Coding Libraries Necessary for Running Subsequent Code"
      ],
      "metadata": {
        "id": "fU6YPxFnnl1Y"
      },
      "id": "fU6YPxFnnl1Y"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "269c6b87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "269c6b87",
        "outputId": "253b34db-3aa1-4839-8462-d5e94b4639f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Dependencies installed & Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install FIRST, then import\n",
        "%pip install -q biopython       # Install the Biopython package quietly (-q suppresses most output) so we can work with biological sequence files\n",
        "\n",
        "from google.colab import drive  # Import the module that lets Colab interact with Google Drive\n",
        "drive.mount('/content/drive')   # Mount your Google Drive so it appears in Colab's file system under /content/drive\n",
        "\n",
        "import os, pandas as pd          # Import 'os' for file/directory operations, and pandas for working with data tables\n",
        "from Bio import SeqIO            # Import SeqIO from Biopython for reading/writing biological sequence files (FASTA, GenBank, etc.)\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib's plotting library to create figures and graphs\n",
        "\n",
        "print(\"‚úÖ Dependencies installed & Drive mounted.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fa44d1",
      "metadata": {
        "id": "42fa44d1"
      },
      "source": [
        "\n",
        "## B. Course folders: Define the course folders for places to load data to be processed and output to be saved\n",
        "\n",
        "Edit only `LECTURE_CODE` and `TOPIC` if needed. All inputs will live in `Data/LECTURE_TOPIC` and outputs in `Outputs/LECTURE_TOPIC`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "66b0e9d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66b0e9d7",
        "outputId": "b3af72b4-fe39-49b7-a0d7-db709f7b12fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ COURSE_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\n",
            "üìÅ DATA_DIR   : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diveristy\n",
            "üìÅ OUTPUT_DIR : /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Outputs/L0-species_diveristy\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Course folder config (customize LECTURE_CODE/TOPIC only) ---\n",
        "COURSE_DIR   = \"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25\"\n",
        "LECTURE_CODE = \"L0-species\"            # change per week (e.g., L02, L03, ...)\n",
        "TOPIC        = \"diveristy\"    # short slug for the exercise\n",
        "\n",
        "# Derived paths (do not change)\n",
        "DATA_DIR   = f\"{COURSE_DIR}/Data/{LECTURE_CODE}_{TOPIC}\"\n",
        "OUTPUT_DIR = f\"{COURSE_DIR}/Outputs/{LECTURE_CODE}_{TOPIC}\"\n",
        "\n",
        "# Create folder structure if missing\n",
        "for p in [f\"{COURSE_DIR}/Data\", f\"{COURSE_DIR}/Outputs\", f\"{COURSE_DIR}/Notebooks\", DATA_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ COURSE_DIR :\", COURSE_DIR)\n",
        "print(\"üìÅ DATA_DIR   :\", DATA_DIR)\n",
        "print(\"üìÅ OUTPUT_DIR :\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C.\n",
        "\n"
      ],
      "metadata": {
        "id": "ITJ9NS_HlbIU"
      },
      "id": "ITJ9NS_HlbIU"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2cd941d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "2cd941d3",
        "outputId": "56d8060a-a7db-49ba-d752-57671a5890be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß¨ Loaded 107 TaxIDs from /content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity/taxids.txt\n",
            "üîé Built 4 ENTREZ_QUERY chunk(s) with [ORGN]\n",
            "‚è≥ BLASTP nr  E=1e-05  hits=100\n",
            "   ENTREZ_QUERY: txid10090[ORGN] OR txid102285[ORGN] OR txid105358[ORGN] OR txid1117[ORGN] OR txid115547[ORGN] OR txid1179[ORGN] OR txid1408545[ORGN] OR txid1416614[ORGN] OR txi ‚Ä¶\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-879294820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEQ\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mrec_xml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_blastp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrez_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhitlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrec_xml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"blastp_nr_chunk{i}_ORGN.xml\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mEQ\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"blastp_nr_noFilter.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-879294820.py\u001b[0m in \u001b[0;36mrun_blastp\u001b[0;34m(query_seq, entrez_query, evalue, hitlist)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentrez_query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" ‚Ä¶\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentrez_query\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentrez_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentrez_query\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"(none)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚è≥ BLASTP nr  E={evalue}  hits={hitlist}\\n   ENTREZ_QUERY: {prev}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     h = NCBIWWW.qblast(\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprogram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blastp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mexpect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrez_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentrez_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py\u001b[0m in \u001b[0;36mqblast\u001b[0;34m(program, database, sequence, url_base, auto_format, composition_based_statistics, db_genetic_code, endpoints, entrez_query, expect, filter, gapcosts, genetic_code, hitlist_size, i_thresh, layout, lcase_mask, matrix_name, nucl_penalty, nucl_reward, other_advanced, perc_ident, phi_pattern, query_file, query_believe_defline, query_from, query_to, searchsp_eff, service, threshold, ungapped_alignment, word_size, short_query, alignments, alignment_view, descriptions, entrez_links_new_window, expect_low, expect_high, format_entrez_query, format_object, format_type, ncbi_gi, results_file, show_overview, megablast, template_type, template_length, username, password)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqblast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0mqblast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- BLASTP vs NR with taxonomy filter from Data/L0-species_diversity ---\n",
        "from Bio import Entrez, SeqIO\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from pathlib import Path\n",
        "import io, csv, re, time, sys\n",
        "\n",
        "# 0) REQUIRED: your email (API key optional)\n",
        "Entrez.email = \"you@university.edu\"      # <- change me\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"   # <- optional but helpful\n",
        "\n",
        "# 1) Resolve paths (use your course vars if present; else fall back to the Data folder in your screenshot)\n",
        "if 'DATA_DIR' in globals():\n",
        "    DATA_DIR = Path(DATA_DIR)\n",
        "else:\n",
        "    DATA_DIR = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\")\n",
        "\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "else:\n",
        "    OUTPUT_DIR = DATA_DIR   # keep outputs next to inputs if course OUTPUT_DIR not set\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2) Pick the protein FASTA (your file is query_proteins.fasta)\n",
        "from pathlib import Path\n",
        "from Bio import SeqIO\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\")\n",
        "FASTA_PATH = DATA_DIR / \"query_proteins.fasta\"   # direct reference\n",
        "\n",
        "rec = next(SeqIO.parse(str(FASTA_PATH), \"fasta\"))\n",
        "seq = str(rec.seq).upper().replace(\"*\",\"\")\n",
        "# 3) Load TaxIDs (prefer Data folder; yours is here)\n",
        "def load_taxids():\n",
        "    for p in (DATA_DIR/\"taxids.txt\", OUTPUT_DIR/\"taxids.txt\"):\n",
        "        if p.exists():\n",
        "            ids = [l.strip() for l in p.read_text().splitlines() if l.strip()]\n",
        "            ids = [t for t in ids if t.isdigit()]\n",
        "            if ids:\n",
        "                print(f\"üß¨ Loaded {len(ids)} TaxIDs from {p}\")\n",
        "                return ids\n",
        "    print(\"‚ö†Ô∏è No taxids.txt found. Running without taxonomy filter.\")\n",
        "    return []\n",
        "\n",
        "taxids = load_taxids()\n",
        "\n",
        "def make_query_chunks(ids, field=\"ORGN\", chunk_size=30):\n",
        "    \"\"\"field='ORGN' is safest for BLAST. Use 'ORGN:exp' to include descendants (may not be supported for all cases).\"\"\"\n",
        "    if not ids: return [None]\n",
        "    chunks = []\n",
        "    for i in range(0, len(ids), chunk_size):\n",
        "        sub = ids[i:i+chunk_size]\n",
        "        q = \" OR \".join([f\"txid{t}[{field}]\" for t in sub])\n",
        "        chunks.append(q)\n",
        "    print(f\"üîé Built {len(chunks)} ENTREZ_QUERY chunk(s) with [{field}]\")\n",
        "    return chunks\n",
        "\n",
        "chunks = make_query_chunks(taxids, field=\"ORGN\", chunk_size=30)\n",
        "\n",
        "# 4) Minimal BLASTP runner\n",
        "def run_blastp(query_seq, entrez_query=None, evalue=1e-5, hitlist=100):\n",
        "    prev = (entrez_query[:160] + \" ‚Ä¶\") if (entrez_query and len(entrez_query)>160) else (entrez_query or \"(none)\")\n",
        "    print(f\"‚è≥ BLASTP nr  E={evalue}  hits={hitlist}\\n   ENTREZ_QUERY: {prev}\")\n",
        "    h = NCBIWWW.qblast(\n",
        "        program=\"blastp\", database=\"nr\", sequence=query_seq,\n",
        "        expect=evalue, entrez_query=entrez_query,\n",
        "        hitlist_size=hitlist, descriptions=hitlist, alignments=hitlist\n",
        "    )\n",
        "    xml = h.read(); h.close()\n",
        "    rec = NCBIXML.read(io.StringIO(xml))\n",
        "    return rec, xml\n",
        "\n",
        "def extract_accession(hit_id, hit_def, accession_attr):\n",
        "    if accession_attr:\n",
        "        return accession_attr\n",
        "    m = re.search(r\"([A-Z]{1,3}_?\\d+\\.\\d+)\", hit_id or \"\")\n",
        "    if m: return m.group(1)\n",
        "    m = re.search(r\"([A-Z]{1,3}_?\\d+\\.\\d+)\", hit_def or \"\")\n",
        "    if m: return m.group(1)\n",
        "    return (hit_id or \"unknown\")\n",
        "\n",
        "# 5) Try chunks with [ORGN]; if still nothing, optional retry with [ORGN:exp]; final fallback is no filter\n",
        "all_alignments, xml_paths = [], []\n",
        "for i, EQ in enumerate(chunks, 1):\n",
        "    try:\n",
        "        rec_xml, xml = run_blastp(seq, entrez_query=EQ, evalue=1e-5, hitlist=100)\n",
        "        if rec_xml.alignments:\n",
        "            path = OUTPUT_DIR / (f\"blastp_nr_chunk{i}_ORGN.xml\" if EQ else \"blastp_nr_noFilter.xml\")\n",
        "            path.write_text(xml); xml_paths.append(path)\n",
        "            all_alignments.extend(rec_xml.alignments)\n",
        "            print(f\"‚úÖ Hits in chunk {i}: {len(rec_xml.alignments)}; saved {path.name}\")\n",
        "            # (keep going to gather from all chunks; comment the next line if you only want the first)\n",
        "            # break\n",
        "        else:\n",
        "            print(f\"‚Äî No hits for chunk {i}\")\n",
        "    except Exception as e:\n",
        "        # Most common message here: \"... is not supported\" if the field syntax is wrong\n",
        "        print(f\"‚ö†Ô∏è Chunk {i} failed: {e}\")\n",
        "    time.sleep(0.25)\n",
        "\n",
        "if not all_alignments and taxids:\n",
        "    print(\"üü° Retrying with descendant expansion [ORGN:exp] (may not be supported for all cases)‚Ä¶\")\n",
        "    for i, EQ in enumerate(make_query_chunks(taxids, field=\"ORGN:exp\", chunk_size=20), 1):\n",
        "        try:\n",
        "            rec_xml, xml = run_blastp(seq, entrez_query=EQ, evalue=1e-5, hitlist=100)\n",
        "            if rec_xml.alignments:\n",
        "                path = OUTPUT_DIR / f\"blastp_nr_chunk{i}_ORGNexp.xml\"\n",
        "                path.write_text(xml); xml_paths.append(path)\n",
        "                all_alignments.extend(rec_xml.alignments)\n",
        "                print(f\"‚úÖ Hits in chunk {i} (exp): {len(rec_xml.alignments)}; saved {path.name}\")\n",
        "            else:\n",
        "                print(f\"‚Äî No hits for chunk {i} (exp)\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Chunk {i} (exp) failed: {e}\")\n",
        "        time.sleep(0.25)\n",
        "\n",
        "if not all_alignments:\n",
        "    print(\"üü° Final fallback: no taxonomy filter\")\n",
        "    try:\n",
        "        rec_xml, xml = run_blastp(seq, entrez_query=None, evalue=1e-3, hitlist=100)\n",
        "        if rec_xml.alignments:\n",
        "            p = OUTPUT_DIR / \"blastp_nr_fallback.xml\"\n",
        "            p.write_text(xml); xml_paths.append(p)\n",
        "            all_alignments.extend(rec_xml.alignments)\n",
        "            print(f\"‚úÖ Fallback hits: {len(rec_xml.alignments)}; saved {p.name}\")\n",
        "        else:\n",
        "            print(\"üî¥ Still no hits.\")\n",
        "    except Exception as e:\n",
        "        print(f\"üî¥ Fallback failed: {e}\")\n",
        "\n",
        "# 6) Save a compact TSV of the best HSP per hit\n",
        "tsv_path = OUTPUT_DIR / \"blastp_summary.tsv\"\n",
        "with open(tsv_path, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f, delimiter=\"\\t\")\n",
        "    w.writerow([\"accession\",\"title\",\"length\",\"evalue\",\"identity\",\"align_len\",\"pct_identity\",\"q_start\",\"q_end\",\"s_start\",\"s_end\"])\n",
        "    for aln in all_alignments:\n",
        "        best = sorted(aln.hsps, key=lambda h: (h.expect, -h.identities))[0]\n",
        "        acc = extract_accession(aln.hit_id, aln.hit_def, getattr(aln, \"accession\", None))\n",
        "        aln_len = best.align_length\n",
        "        pct_id = round(100.0 * best.identities / aln_len, 2) if aln_len else \"\"\n",
        "        q0, q1 = best.query_start, best.query_end\n",
        "        s0, s1 = best.sbjct_start, best.sbjct_end\n",
        "        w.writerow([acc, aln.title, aln.length, best.expect, best.identities, aln_len, pct_id, min(q0,q1), max(q0,q1), min(s0,s1), max(s0,s1)])\n",
        "\n",
        "print(f\"\\nüì¶ Saved {len(all_alignments)} alignments across {len(xml_paths)} XML file(s).\")\n",
        "print(\"üíæ XML files :\", \", \".join(p.name for p in xml_paths) if xml_paths else \"(none)\")\n",
        "print(\"üìë Summary   :\", tsv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " multi-FASTA of top hits per TaxID (one or more sequences per taxon). The cell below:\n",
        "\t‚Ä¢\tuses your existing folders (Data/L0-species_diversity for input; writes to the same folder unless OUTPUT_DIR is already set),\n",
        "\t‚Ä¢\treads query_proteins.fasta and taxids.txt,\n",
        "\t‚Ä¢\tfor each TaxID, runs a separate BLAST restricted to that taxon (txid####[ORGN]), so we can attribute hits unambiguously,\n",
        "\t‚Ä¢\tgrabs the top N accessions from each BLAST,\n",
        "\t‚Ä¢\tfetches their protein FASTA sequences,\n",
        "\t‚Ä¢\twrites:\n",
        "\t‚Ä¢\tper_taxid_top_hits.fasta (all sequences, grouped by taxid in headers),\n",
        "\t‚Ä¢\tper_taxid_hits.tsv (who came from which taxid, evalue, %id, etc.),\n",
        "\t‚Ä¢\toptional one FASTA per taxid (toggle with WRITE_SPLIT_FASTA)."
      ],
      "metadata": {
        "id": "ADIsRNvmtr3K"
      },
      "id": "ADIsRNvmtr3K"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLAST (per TaxID) -> collect top protein hits -> write multi-FASTA + TSV ---\n",
        "from Bio import Entrez, SeqIO\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from pathlib import Path\n",
        "import io, csv, re, time, sys\n",
        "\n",
        "# ==== config you may tweak ====\n",
        "Entrez.email = \"you@university.edu\"   # <-- REQUIRED\n",
        "# Entrez.api_key = \"YOUR_NCBI_API_KEY\"  # optional, faster/higher limits\n",
        "\n",
        "TOP_HITS_PER_TAXID = 3        # how many sequences to keep per taxid\n",
        "EVALUE              = 1e-5\n",
        "HITLIST_SIZE        = max(50, TOP_HITS_PER_TAXID*10)  # ask for more, then trim\n",
        "WRITE_SPLIT_FASTA   = False   # also write one FASTA per taxid\n",
        "SLEEP_BETWEEN_CALLS = 0.3     # be kind to NCBI :)\n",
        "\n",
        "# ==== paths (use your course vars if present) ====\n",
        "if 'DATA_DIR' in globals():\n",
        "    DATA_DIR = Path(DATA_DIR)\n",
        "else:\n",
        "    DATA_DIR = Path(\"/content/drive/MyDrive/Teaching/BIOINFO4-5203-F25/Data/L0-species_diversity\")\n",
        "\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "else:\n",
        "    OUTPUT_DIR = DATA_DIR\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Files we expect from your screenshot:\n",
        "PROT_FASTA = DATA_DIR / \"query_proteins.fasta\"\n",
        "TAXIDS_TXT = DATA_DIR / \"taxids.txt\"\n",
        "\n",
        "# ==== load query; auto-detect protein vs nucleotide ====\n",
        "def load_query():\n",
        "    if PROT_FASTA.exists():\n",
        "        rec = next(SeqIO.parse(str(PROT_FASTA), \"fasta\"))\n",
        "        seq = str(rec.seq).upper().replace(\"*\",\"\")\n",
        "        letters = [c for c in seq if c.isalpha()]\n",
        "        dna_frac = sum(c in set(\"ACGTN\") for c in letters) / max(1,len(letters))\n",
        "        if dna_frac >= 0.85:\n",
        "            # looks like nucleotide despite filename; fallback to query.fasta if present\n",
        "            alt = DATA_DIR / \"query.fasta\"\n",
        "            if alt.exists():\n",
        "                r2 = next(SeqIO.parse(str(alt), \"fasta\"))\n",
        "                return (\"blastx\", str(r2.seq).upper(), f\"{alt.name}:{r2.id}\")\n",
        "            # else: we'll still do blastx on given sequence\n",
        "            return (\"blastx\", seq, f\"{PROT_FASTA.name}:{rec.id}\")\n",
        "        else:\n",
        "            return (\"blastp\", seq, f\"{PROT_FASTA.name}:{rec.id}\")\n",
        "    else:\n",
        "        # if protein file missing, try nucleotide and use blastx\n",
        "        alt = DATA_DIR / \"query.fasta\"\n",
        "        if alt.exists():\n",
        "            r2 = next(SeqIO.parse(str(alt), \"fasta\"))\n",
        "            return (\"blastx\", str(r2.seq).upper(), f\"{alt.name}:{r2.id}\")\n",
        "        raise FileNotFoundError(f\"Could not find {PROT_FASTA} or a fallback nucleotide FASTA at {DATA_DIR/'query.fasta'}\")\n",
        "\n",
        "mode, query_seq, query_label = load_query()\n",
        "print(f\"üìÑ Query source: {query_label}\")\n",
        "print(f\"üß™ Mode chosen: {mode.upper()} vs nr  | length={len(query_seq)}\")\n",
        "\n",
        "# ==== load TaxIDs ====\n",
        "if not TAXIDS_TXT.exists():\n",
        "    raise FileNotFoundError(f\"taxids.txt not found at {TAXIDS_TXT}\")\n",
        "taxids = [t.strip() for t in TAXIDS_TXT.read_text().splitlines() if t.strip().isdigit()]\n",
        "if not taxids:\n",
        "    raise ValueError(\"taxids.txt is empty or contains no numeric TaxIDs.\")\n",
        "print(f\"üß¨ Loaded {len(taxids)} TaxIDs\")\n",
        "\n",
        "# ==== helpers ====\n",
        "def run_single_blast(seq, taxid, program=\"blastp\", evalue=EVALUE, hitlist=HITLIST_SIZE):\n",
        "    q = f\"txid{taxid}[ORGN]\"  # exact taxon; robust for BLAST\n",
        "    prev = q if len(q) < 160 else (q[:157] + \" ‚Ä¶\")\n",
        "    print(f\"‚è≥ {program.upper()} vs nr | taxid={taxid} | E={evalue} | hits={hitlist}\\n   ENTREZ_QUERY: {prev}\")\n",
        "    h = NCBIWWW.qblast(program=program, database=\"nr\", sequence=seq,\n",
        "                       expect=evalue, entrez_query=q,\n",
        "                       hitlist_size=hitlist, descriptions=hitlist, alignments=hitlist)\n",
        "    xml = h.read(); h.close()\n",
        "    rec = NCBIXML.read(io.StringIO(xml))\n",
        "    return rec, xml\n",
        "\n",
        "def extract_accession(hit_id, hit_def, accession_attr):\n",
        "    if accession_attr: return accession_attr\n",
        "    m = re.search(r\"([A-Z]{1,3}_?\\d+\\.\\d+)\", hit_id or \"\")\n",
        "    if m: return m.group(1)\n",
        "    m = re.search(r\"([A-Z]{1,3}_?\\d+\\.\\d+)\", hit_def or \"\")\n",
        "    if m: return m.group(1)\n",
        "    return (hit_id or \"unknown\")\n",
        "\n",
        "def fetch_protein_fasta(accessions):\n",
        "    \"\"\"Return dict acc->FASTA text (single-record each).\"\"\"\n",
        "    out = {}\n",
        "    batch = list(accessions)\n",
        "    while batch:\n",
        "        # fetch in small batches to be polite\n",
        "        chunk = batch[:50]; batch = batch[50:]\n",
        "        try:\n",
        "            h = Entrez.efetch(db=\"protein\", id=\",\".join(chunk), rettype=\"fasta\", retmode=\"text\")\n",
        "            txt = h.read(); h.close()\n",
        "            # split on records\n",
        "            parts = [t for t in txt.strip().split(\">\") if t]\n",
        "            for rec_txt in parts:\n",
        "                header, *seq_lines = rec_txt.splitlines()\n",
        "                acc = header.split()[0]\n",
        "                out[acc] = \">\" + header + \"\\n\" + \"\\n\".join(seq_lines) + \"\\n\"\n",
        "        except Exception as e:\n",
        "            sys.stderr.write(f\"[warn] efetch failed for {chunk}: {e}\\n\")\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "    return out\n",
        "\n",
        "# ==== main loop: BLAST per TaxID -> keep top N -> fetch FASTA ====\n",
        "all_rows = []\n",
        "per_taxid_fastas = {}   # taxid -> list of FASTA strings\n",
        "xml_paths = []\n",
        "\n",
        "for i, tid in enumerate(taxids, 1):\n",
        "    try:\n",
        "        record, xml = run_single_blast(query_seq, tid, program=(\"blastp\" if mode==\"blastp\" else \"blastx\"))\n",
        "        # save each XML (handy for grading/debug)\n",
        "        xml_file = OUTPUT_DIR / f\"{mode}_nr_taxid{tid}.xml\"\n",
        "        xml_file.write_text(xml); xml_paths.append(xml_file)\n",
        "\n",
        "        if not record.alignments:\n",
        "            print(f\"‚Äî No hits for taxid {tid}\")\n",
        "            continue\n",
        "\n",
        "        # pick top-N by (evalue, -identities)\n",
        "        hsps = []\n",
        "        for aln in record.alignments:\n",
        "            best = sorted(aln.hsps, key=lambda h: (h.expect, -h.identities))[0]\n",
        "            acc = extract_accession(aln.hit_id, aln.hit_def, getattr(aln, \"accession\", None))\n",
        "            pct_id = 100.0 * best.identities / best.align_length if best.align_length else 0.0\n",
        "            hsps.append((aln, best, acc, pct_id))\n",
        "        hsps.sort(key=lambda t: (t[1].expect, -t[3]))\n",
        "        keep = hsps[:TOP_HITS_PER_TAXID]\n",
        "\n",
        "        # fetch FASTAs for these accessions\n",
        "        accs = [acc for _,_,acc,_ in keep]\n",
        "        acc_to_fa = fetch_protein_fasta(accs)\n",
        "\n",
        "        # store\n",
        "        per_taxid_fastas.setdefault(tid, [])\n",
        "        for aln, best, acc, pct in keep:\n",
        "            fa = acc_to_fa.get(acc)\n",
        "            if not fa:\n",
        "                continue\n",
        "            # prepend taxid info to header for grouping\n",
        "            # keep original header after a pipe\n",
        "            lines = fa.strip().splitlines()\n",
        "            header = lines[0][1:]  # drop '>'\n",
        "            seq = \"\\n\".join(lines[1:])\n",
        "            new_header = f\">taxid:{tid}|acc:{acc}|e:{best.expect:.2e}|pid:{pct:.2f}|len:{best.align_length} {header}\"\n",
        "            per_taxid_fastas[tid].append(new_header + \"\\n\" + seq + \"\\n\")\n",
        "\n",
        "            all_rows.append([\n",
        "                tid, acc, aln.title, aln.length, best.expect,\n",
        "                best.identities, best.align_length, round(pct,2),\n",
        "                min(best.query_start,best.query_end), max(best.query_start,best.query_end),\n",
        "                min(best.sbjct_start,best.sbjct_end), max(best.sbjct_start,best.sbjct_end)\n",
        "            ])\n",
        "        print(f\"‚úÖ taxid {tid}: kept {len(per_taxid_fastas[tid])} sequences\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è taxid {tid} failed: {e}\")\n",
        "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "\n",
        "# ==== write combined multi-FASTA + per-taxid FASTAs ====\n",
        "multi_fa = OUTPUT_DIR / \"per_taxid_top_hits.fasta\"\n",
        "with open(multi_fa, \"w\") as fh:\n",
        "    for tid in taxids:\n",
        "        for fa in per_taxid_fastas.get(tid, []):\n",
        "            fh.write(fa)\n",
        "print(f\"üíæ Multi-FASTA: {multi_fa}\")\n",
        "\n",
        "if WRITE_SPLIT_FASTA:\n",
        "    for tid, fas in per_taxid_fastas.items():\n",
        "        p = OUTPUT_DIR / f\"taxid_{tid}_top_hits.fasta\"\n",
        "        with open(p, \"w\") as fh:\n",
        "            for fa in fas: fh.write(fa)\n",
        "    print(\"üíæ Also wrote per-taxid FASTAs\")\n",
        "\n",
        "# ==== write table ====\n",
        "tsv = OUTPUT_DIR / \"per_taxid_hits.tsv\"\n",
        "with open(tsv, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f, delimiter=\"\\t\")\n",
        "    w.writerow([\"taxid\",\"accession\",\"title\",\"subject_length\",\"evalue\",\"identities\",\"align_len\",\"pct_identity\",\"q_start\",\"q_end\",\"s_start\",\"s_end\"])\n",
        "    w.writerows(all_rows)\n",
        "print(f\"üìë Table: {tsv}\")\n",
        "\n",
        "print(f\"üóÇ XML files saved: {len(xml_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSGXBMA5tc72",
        "outputId": "52f20778-ee0e-4262-f05b-8054665017fa"
      },
      "id": "tSGXBMA5tc72",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Query source: query_proteins.fasta:unknown_seq\n",
            "üß™ Mode chosen: BLASTP vs nr  | length=475\n",
            "üß¨ Loaded 107 TaxIDs\n",
            "‚è≥ BLASTP vs nr | taxid=10090 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid10090[ORGN]\n",
            "‚úÖ taxid 10090: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=102285 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid102285[ORGN]\n",
            "‚úÖ taxid 102285: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=105358 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid105358[ORGN]\n",
            "‚Äî No hits for taxid 105358\n",
            "‚è≥ BLASTP vs nr | taxid=1117 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1117[ORGN]\n",
            "‚úÖ taxid 1117: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=115547 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid115547[ORGN]\n",
            "‚úÖ taxid 115547: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1179 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1179[ORGN]\n",
            "‚úÖ taxid 1179: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1408545 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1408545[ORGN]\n",
            "‚úÖ taxid 1408545: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1416614 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1416614[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request A2EG6BN2015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ taxid 1416614: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1552121 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1552121[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request A2F79E6J015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ taxid 1552121: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=159855 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid159855[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request A2FUNYP3015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ taxid 159855: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1644118 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1644118[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request A2GFYWPU015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ taxid 1644118: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1673428 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1673428[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request A2H7261X015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚Äî No hits for taxid 1673428\n",
            "‚è≥ BLASTP vs nr | taxid=172827 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid172827[ORGN]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Blast/NCBIWWW.py:275: BiopythonWarning: BLAST request A2HUCW6B015 is taking longer than 10 minutes, consider re-issuing it\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ taxid 172827: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1737569 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1737569[ORGN]\n",
            "‚úÖ taxid 1737569: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1752064 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1752064[ORGN]\n",
            "‚úÖ taxid 1752064: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1761908 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1761908[ORGN]\n",
            "‚úÖ taxid 1761908: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=186192 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid186192[ORGN]\n",
            "‚úÖ taxid 186192: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=187137 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid187137[ORGN]\n",
            "‚úÖ taxid 187137: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1874737 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1874737[ORGN]\n",
            "‚úÖ taxid 1874737: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1904752 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1904752[ORGN]\n",
            "‚úÖ taxid 1904752: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1906666 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1906666[ORGN]\n",
            "‚úÖ taxid 1906666: kept 0 sequences\n",
            "‚è≥ BLASTP vs nr | taxid=1906667 | E=1e-05 | hits=50\n",
            "   ENTREZ_QUERY: txid1906667[ORGN]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}